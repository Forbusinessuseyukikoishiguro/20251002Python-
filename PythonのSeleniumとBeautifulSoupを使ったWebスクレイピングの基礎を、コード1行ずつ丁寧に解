はじめに

この記事では、PythonのSeleniumとBeautifulSoupを使ったWebスクレイピングの基礎を、コード1行ずつ丁寧に解説します。実際に動くコードを通じて、Webページの構造分析からデータ抽出までを学びましょう！

1. 必要なライブラリとインポート

コード全体（インポート部分）

# === 必要なライブラリを全てインポート ===
# Seleniumの基本機能
from selenium import webdriver
# Chromeのオプション設定用
from selenium.webdriver.chrome.options import Options
# Chromeのサービス設定用
from selenium.webdriver.chrome.service import Service
# ChromeDriverを自動ダウンロードするツール
from webdriver_manager.chrome import ChromeDriverManager
# HTML要素を探すための方法を指定
from selenium.webdriver.common.by import By
# BeautifulSoupでHTML解析
from bs4 import BeautifulSoup
# 待機処理用
import time
# ログ出力用
import logging


1行ずつ解説

# Seleniumの基本機能
from selenium import webdriver


解説: Seleniumのメインモジュール。ブラウザを自動操作するための基本機能を提供します。

# Chromeのオプション設定用
from selenium.webdriver.chrome.options import Options


解説: Chromeブラウザの動作設定（ヘッドレスモード、ウィンドウサイズなど）を指定するクラス。

# Chromeのサービス設定用
from selenium.webdriver.chrome.service import Service


解説: ChromeDriverのパスや起動オプションを管理するクラス。

# ChromeDriverを自動ダウンロードするツール
from webdriver_manager.chrome import ChromeDriverManager


解説: ChromeDriverを手動でダウンロードせず、自動的に適切なバージョンを取得・管理してくれる便利なツール。

# HTML要素を探すための方法を指定
from selenium.webdriver.common.by import By


解説: HTML要素を検索する方法（ID、クラス名、CSSセレクターなど）を指定するための定数を提供。

# BeautifulSoupでHTML解析
from bs4 import BeautifulSoup


解説: HTMLを解析して、要素の抽出や操作を簡単にするライブラリ。Seleniumで取得したHTMLを整形・検索するのに使用。

# 待機処理用
import time


解説: プログラムを一時停止させるための標準ライブラリ。ページ読み込み待機などに使用。

# ログ出力用
import logging


解説: プログラムの実行状況や エラーを記録するための標準ライブラリ。デバッグやトラブルシューティングに必須。

2. ログ設定の重要性

# ログの設定
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


1行ずつ解説

logging.basicConfig(


解説: ログの基本設定を行う関数。プログラム全体のログ動作を制御します。

    level=logging.INFO,


解説: ログレベルをINFOに設定。DEBUG（詳細）、INFO（通常）、WARNING（警告）、ERROR（エラー）、CRITICAL（致命的）の5段階があり、INFO以上のログが出力されます。

    format='%(asctime)s - %(levelname)s - %(message)s'


解説: ログの出力形式を指定。

%(asctime)s: タイムスタンプ（例: 2025-10-04 15:05:23,886）

%(levelname)s: ログレベル（INFO、ERROR など）

%(message)s: ログメッセージ本文

logger = logging.getLogger(__name__)


解説: このファイル専用のロガーオブジェクトを作成。__name__は現在のモジュール名が入ります。

3. ドライバーのセットアップ

def find_specific_elements(url):
    """
    特定のパターンの要素を探索する
    """


解説

def find_specific_elements(url):


解説: 関数定義。引数urlには分析したいWebページのURLを渡します。

    """
    特定のパターンの要素を探索する
    """


解説: Docstring（ドキュメント文字列）。関数の説明を記述します。他の開発者や将来の自分が見てもわかるように書きます。

Chromeオプションの設定

    # Chromeのオプションを作成
    chrome_options = Options()


解説: Chromeブラウザの設定を格納するオブジェクトを作成。この後、様々な設定を追加していきます。

    # ヘッドレスモードをオフ(画面を表示)
    # chrome_options.add_argument('--headless')  # コメントアウト=画面表示


解説:

--headless: ブラウザを画面に表示せずに実行するモード

コメントアウトされているので、現在は画面が表示される設定

コメントを外すと画面非表示で高速実行できます

    # その他の基本設定
    chrome_options.add_argument('--disable-gpu')  # GPU無効化


解説: GPU（グラフィック処理）を無効化。ヘッドレスモードで動作を安定させるため。

    chrome_options.add_argument('--no-sandbox')  # サンドボックス無効化


解説: サンドボックス（セキュリティ保護機能）を無効化。Linuxやコンテナ環境で必要な場合があります。

    chrome_options.add_argument('--window-size=1920,1080')  # ウィンドウサイズ


解説: ブラウザのウィンドウサイズを1920x1080ピクセルに設定。レスポンシブデザインのサイトで重要。

    # User-Agentを設定(ボット判定回避)
    chrome_options.add_argument(
        'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    )


解説: User-Agent（ブラウザ識別情報）を設定。通常のブラウザとして認識されるようにして、ボット判定を回避します。

ChromeDriverの起動

    # ChromeDriverを自動インストール
    service = Service(ChromeDriverManager().install())


解説:

ChromeDriverManager().install(): ChromeDriverを自動ダウンロード・インストール

Service(): ダウンロードしたドライバーでサービスオブジェクトを作成

    # Chromeドライバーを起動
    driver = webdriver.Chrome(service=service, options=chrome_options)


解説:

webdriver.Chrome(): Chromeブラウザを起動

service: 先ほど作成したサービス（ドライバー情報）を渡す

options: 設定したオプションを適用

4. ページへのアクセスとHTML取得

    try:


解説: 例外処理の開始。この中でエラーが発生した場合、exceptブロックで処理されます。

        # 対象URLにアクセス
        logger.info(f"アクセス中: {url}")


解説: INFOレベルのログを出力。f"..." はf-string（フォーマット済み文字列）で、変数を埋め込めます。

        driver.get(url)


解説: 指定されたURLにブラウザでアクセス。この時点でページが開かれます。

        # ページが完全に読み込まれるまで3秒待機
        time.sleep(3)


解説: 3秒間プログラムを停止。JavaScriptで動的に生成されるコンテンツの読み込みを待ちます。

        # ページのHTMLソースを取得
        page_source = driver.page_source


解説: 現在表示されているページの完全なHTMLソースコードを文字列として取得。

        # BeautifulSoupで解析(HTMLを扱いやすくする)
        soup = BeautifulSoup(page_source, 'html.parser')


解説:

BeautifulSoup(): HTMLを解析してオブジェクト化

page_source: 解析するHTML文字列

'html.parser': 使用するパーサー（解析エンジン）

5. HTML要素の検索

        print("\n" + "="*80)
        print("【coorikuya.com 詳細な要素検索】")
        print("="*80)


解説:

"\n": 改行

"="*80: =を80回繰り返して区切り線を作成

見やすい出力のための装飾

パターン1: 記事一覧を探す

        # === パターン1: 記事一覧を探す ===
        print("\n1. 記事一覧候補:")


解説: セクションの開始を表示。

        # よくある記事一覧のパターン(CSSセレクター)
        patterns = [
            ('article'),  # articleタグ
            ('.post'),  # postクラス
            ('.entry'),  # entryクラス
            ('.blog-post'),  # blog-postクラス
            ('.article-item'),  # article-itemクラス
            ('[class*="post"]'),  # postを含むクラス
            ('.card'),  # cardクラス
            ('.item'),  # itemクラス
        ]


解説:

リスト型の変数を定義

CSSセレクター形式で検索パターンを列挙

article: タグ名で検索

.post: クラス名で検索（ドット付き）

[class*="post"]: 部分一致検索（postを含むクラス）

        # 各パターンで要素を検索
        for pattern in patterns:


解説: patternsリストの各要素を順番にpattern変数に格納してループ処理。

            # CSSセレクターで要素を検索
            elements = soup.select(pattern)


解説:

soup.select(): CSSセレクターで要素を検索

該当する全ての要素をリストで返す

見つからない場合は空リスト[]

            # 要素が見つかった場合
            if elements:


解説: リストが空でない場合（要素が1つ以上見つかった場合）に処理を実行。

                print(f"  ✓ セレクター '{pattern}': {len(elements)}個見つかりました")


解説:

len(elements): リストの要素数を取得

見つかった要素の数を表示

                # 最初の要素の構造を表示
                if elements:
                    first = elements[0]


解説:

elements[0]: リストの最初の要素を取得（インデックスは0から始まる）

first変数に格納

                    # タグ名とクラス名を取得
                    class_list = first.get('class', [])


解説:

first.get('class', []): 要素のclass属性を取得

存在しない場合は空リスト[]を返す

                    print(f"    構造例: <{first.name} class='{' '.join(class_list)}'>")


解説:

first.name: タグ名（例: div, article）

' '.join(class_list): リストをスペース区切りの文字列に変換

例: ['item', 'active'] → 'item active'

                    # 直接の子要素を確認
                    children = first.find_all(recursive=False)


解説:

find_all(): 全ての子孫要素を検索

recursive=False: 直接の子要素のみ（孫要素は含まない）

                    print(f"    直接の子要素: {len(children)}個")


解説: 子要素の数を表示。

パターン2: タイトルを探す

        # === パターン2: タイトルを探す ===
        print("\n2. タイトル候補:")
        title_patterns = [
            ('h1'),  # h1タグ
            ('h2'),  # h2タグ
            ('h2.entry-title'),  # entry-titleクラスを持つh2
            ('h2.post-title'),  # post-titleクラスを持つh2
            ('.title'),  # titleクラス
            ('article h2'),  # article内のh2
            ('h2 a'),  # リンクを含むh2
            ('.entry-title'),  # entry-titleクラス
        ]


解説:

タイトル要素を探すパターンのリスト

h2.entry-title: タグとクラスの組み合わせ

article h2: 子孫セレクター（article内のh2）

h2 a: h2内のaタグ

        # 各パターンで検索
        for pattern in title_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  ✓ '{pattern}': {len(elements)}個")


解説: タイトルパターンでループして検索結果を表示。

                # 最初のテキストを表示(50文字まで)
                if elements:
                    text = elements[0].get_text(strip=True)


解説:

get_text(): 要素内のテキストを取得（HTMLタグを除去）

strip=True: 前後の空白・改行を削除

                    print(f"    例: {text[:50]}")


解説:

text[:50]: 文字列の最初の50文字を取得（スライス）

長いテキストの場合は省略表示

パターン3: 日付を探す

        # === パターン3: 日付を探す ===
        print("\n3. 日付候補:")
        date_patterns = [
            ('time'),  # timeタグ
            ('.date'),  # dateクラス
            ('.published'),  # publishedクラス
            ('.entry-date'),  # entry-dateクラス
            ('[datetime]'),  # datetime属性を持つ要素
            ('.post-date'),  # post-dateクラス
        ]


解説:

日付要素を探すパターン

[datetime]: 属性セレクター（datetime属性を持つ要素）

        # 各パターンで検索
        for pattern in date_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  ✓ '{pattern}': {len(elements)}個")
                if elements:
                    # テキストまたはdatetime属性を表示
                    text = elements[0].get_text(strip=True)
                    datetime_attr = elements[0].get('datetime', '')


解説:

テキストとdatetime属性の両方を取得

datetime属性は機械可読な日時形式（例: 2025-10-04T15:05:23）

                    print(f"    例: {text}")
                    if datetime_attr:
                        print(f"    datetime属性: {datetime_attr}")


解説: datetime属性がある場合のみ表示。

パターン4: 本文・抜粋を探す

        # === パターン4: 本文・抜粋を探す ===
        print("\n4. 本文・抜粋候補:")
        content_patterns = [
            ('.entry-content'),  # entry-contentクラス
            ('.post-content'),  # post-contentクラス
            ('.excerpt'),  # excerptクラス
            ('.summary'),  # summaryクラス
            ('article p'),  # article内のp(段落)
            ('.description'),  # descriptionクラス
        ]


解説: 本文や要約を含む要素のパターン。

        # 各パターンで検索
        for pattern in content_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  ✓ '{pattern}': {len(elements)}個")
                if elements:
                    # テキストを取得(最初の80文字)
                    text = elements[0].get_text(strip=True)
                    print(f"    例: {text[:80]}...")


解説: 本文の最初の80文字を表示。...で省略を示します。

パターン5: カテゴリー・タグを探す

        # === パターン5: カテゴリー・タグを探す ===
        print("\n5. カテゴリー・タグ候補:")
        category_patterns = [
            ('.category'),  # categoryクラス
            ('.tag'),  # tagクラス
            ('.categories'),  # categoriesクラス
            ('.tags'),  # tagsクラス
            ('a[rel="category"]'),  # rel属性がcategoryのリンク
            ('a[rel="tag"]'),  # rel属性がtagのリンク
            ('.cat-links'),  # cat-linksクラス
        ]


解説:

カテゴリーやタグを探すパターン

a[rel="category"]: 属性値でフィルタリング

        # 各パターンで検索
        for pattern in category_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  ✓ '{pattern}': {len(elements)}個")
                if elements:
                    # テキストを表示
                    print(f"    例: {elements[0].get_text(strip=True)}")


解説: カテゴリー名を表示。

パターン6: 画像を探す

        # === パターン6: 画像を探す ===
        print("\n6. 画像候補:")
        images = soup.find_all('img')


解説:

find_all('img'): 全ての<img>タグを検索

CSSセレクターではなくタグ名で直接検索

        print(f"  総画像数: {len(images)}個")


解説: 画像の総数を表示。

        # 最初の3つの画像を表示
        for i, img in enumerate(images[:3], 1):


解説:

images[:3]: 最初の3つの要素

enumerate(..., 1): インデックスと要素を同時に取得、番号は1から開始

            src = img.get('src', '')
            alt = img.get('alt', '')


解説:

src: 画像のURL

alt: 代替テキスト（画像が表示されない時に表示される説明文）

            print(f"  {i}. alt='{alt[:30]}' src='{src[:60]}'")


解説: 画像情報を表示。長い場合は省略。

パターン7: リンクを探す

        # === パターン7: リンクを探す ===
        print("\n7. リンク候補:")
        links = soup.find_all('a', href=True)


解説:

find_all('a', href=True): href属性を持つ<a>タグのみ検索

hrefがないリンクは除外される

        print(f"  総リンク数: {len(links)}個")
        
        # 最初の5つのリンクを表示
        for i, link in enumerate(links[:5], 1):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            print(f"  {i}. {text[:30]} -> {href[:50]}")


解説: リンクのテキストとURLを表示。

6. HTML構造ツリーの表示

        # === 全体のHTML構造ツリーを表示 ===
        print("\n8. HTML構造ツリー(body直下の要素):")
        body = soup.find('body')


解説:

find('body'): 最初の<body>タグを検索

find_all()と異なり、1つだけ返す

        if body:
            # bodyの直接の子要素を列挙
            for i, child in enumerate(body.find_all(recursive=False), 1):


解説:

bodyが存在する場合のみ処理

recursive=False: 直接の子要素のみ

                # クラス名を取得
                class_str = ' '.join(child.get('class', []))
                # ID属性を取得
                id_str = child.get('id', '')


解説: クラス名とID属性を取得。

                # 出力
                print(f"  {i}. <{child.name}> ", end='')


解説:

end='': 改行せずに次の出力を続ける

デフォルトはend='\n'（改行）

                if id_str:
                    print(f"id='{id_str}' ", end='')
                if class_str:
                    print(f"class='{class_str}'", end='')
                print()


解説:

ID、クラスがある場合のみ表示

最後のprint()で改行

7. クラス名の頻出ランキング

        # === よく使われているクラス名のランキング ===
        print("\n9. 頻出クラス名トップ10:")
        class_counter = {}


解説:

空の辞書を作成

クラス名をキー、出現回数を値として格納

        # 全てのタグからクラス名を収集
        for tag in soup.find_all(class_=True):


解説:

class_=True: クラス属性を持つ全ての要素を検索

classはPythonの予約語なのでclass_とする

            classes = tag.get('class', [])
            for cls in classes:
                # カウント
                class_counter[cls] = class_counter.get(cls, 0) + 1


解説:

1つの要素が複数のクラスを持つ場合もあるのでループ

get(cls, 0): 辞書に存在しない場合は0、存在する場合は現在の値

+1: カウントアップ

        # 出現回数でソート
        sorted_classes = sorted(
            class_counter.items(),
            key=lambda x: x[1],
            reverse=True
        )


解説:

class_counter.items(): 辞書を(キー, 値)のタプルのリストに変換

key=lambda x: x[1]: ソートのキー（値=出現回数）

reverse=True: 降順（多い順）

        # トップ10を表示
        for i, (cls, count) in enumerate(sorted_classes[:10], 1):
            print(f"  {i:2d}. '{cls}' - {count}回")


解説:

sorted_classes[:10]: 最初の10個

(cls, count): タプルのアンパック（分解）

{i:2d}: 2桁の整数として表示（右寄せ）

8. HTMLソースとスクリーンショットの保存

        # === HTMLソースを保存 ===
        print("\n10. HTMLソース保存:")
        with open('coorikuya_source.html', 'w', encoding='utf-8') as f:


解説:

with open(): ファイルを開く（自動でクローズされる）

'w': 書き込みモード

encoding='utf-8': UTF-8エンコーディング（日本語対応）

as f: ファイルオブジェクトをfとして扱う

            # きれいに整形して保存
            f.write(soup.prettify())


解説:

soup.prettify(): HTMLをインデント付きで整形

f.write(): ファイルに書き込み

        print("  ✓ 'coorikuya_source.html' に保存しました")


解説: 完了メッセージを表示。

        # === スクリーンショット保存 ===
        driver.save_screenshot('coorikuya_screenshot.png')
        print("  ✓ 'coorikuya_screenshot.png' に保存しました")


解説:

save_screenshot(): 現在のブラウザ画面をPNG画像として保存

デバッグやレポート作成に便利

        print("\n" + "="*80)
        print("【分析完了！】")
        print("="*80)


解説: 処理完了のメッセージ。

9. エラーハンドリング

    except Exception as e:


解説:

tryブロック内で発生したあらゆる例外をキャッチ

as e: 例外オブジェクトをeとして取得

        # エラーが発生した場合
        logger.error(f"エラーが発生しました: {e}")


解説: エラーレベルのログを出力。

        # 詳細なエラー情報を表示
        import traceback
        traceback.print_exc()


解説:

traceback: エラーの詳細情報を扱うモジュール

print_exc(): スタックトレース（エラーが発生した場所と経路）を表示

10. クリーンアップ処理

    finally:


解説:

tryブロックの後、必ず実行される

エラーの有無に関わらず実行されるクリーンアップ処理に最適

        # 必ずブラウザを閉じる
        print("\n5秒後にブラウザを閉じます...")
        time.sleep(5)


解説: ユーザーが結果を確認できるよう5秒待機。

【新人エンジニア向け技術記事】続き

        driver.quit()


解説:

ブラウザを完全に終了

close()は現在のタブのみ閉じるが、quit()は全てのウィンドウとドライバーを終了

メモリリークを防ぐため必ず実行すべき

        logger.info("ブラウザを閉じました")


解説: ブラウザ終了をログに記録。

11. メイン実行部分

# このファイルが直接実行された場合のみ実行
if __name__ == "__main__":


解説:

__name__: 特殊変数。ファイルが直接実行された場合は"__main__"

他のファイルからインポートされた場合は実行されない

これにより、モジュールとしても使えるコードになる

    # coorikuya.comを分析
    find_specific_elements("https://www.coorikuya.com/")


解説:

関数を実行

URLを引数として渡す

📊 重要な概念まとめ

1. CSSセレクターの種類

セレクター 意味 例 tag タグ名 div, article .class クラス名 .post, .item #id ID #header, #main [attr] 属性を持つ [href], [datetime] [attr="value"] 属性値が一致 [rel="category"] [attr*="value"] 属性値に含む [class*="post"] parent child 子孫 article p parent > child 直接の子 div > p

2. BeautifulSoupの主要メソッド

# 1つだけ取得
element = soup.find('div')
element = soup.find('div', class_='post')
element = soup.find('div', id='main')

# 複数取得
elements = soup.find_all('div')
elements = soup.find_all('div', class_='post')
elements = soup.find_all('a', href=True)

# CSSセレクター（推奨）
element = soup.select_one('.post')  # 1つだけ
elements = soup.select('.post')  # 全て

# テキスト取得
text = element.text
text = element.get_text(strip=True)

# 属性取得
href = element.get('href')
src = element.get('src', '')  # デフォルト値指定


3. Seleniumの主要メソッド

# ページ操作
driver.get(url)  # ページを開く
driver.back()  # 戻る
driver.forward()  # 進む
driver.refresh()  # 更新

# 要素検索（1つ）
element = driver.find_element(By.ID, 'element-id')
element = driver.find_element(By.CLASS_NAME, 'class-name')
element = driver.find_element(By.CSS_SELECTOR, '.class')

# 要素検索（複数）
elements = driver.find_elements(By.TAG_NAME, 'div')
elements = driver.find_elements(By.CSS_SELECTOR, '.post')

# 要素操作
element.click()  # クリック
element.send_keys('text')  # テキスト入力
element.clear()  # クリア

# 情報取得
text = element.text
value = element.get_attribute('href')


🔧 実践的なカスタマイズ例

例1: 待機処理の改善

from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# 明示的な待機（推奨）
# 要素が表示されるまで最大10秒待つ
try:
    # WebDriverWaitオブジェクトを作成（最大10秒待機）
    wait = WebDriverWait(driver, 10)
    
    # 要素が存在するまで待つ
    element = wait.until(
        EC.presence_of_element_located((By.CLASS_NAME, 'item'))
    )
    
    # 要素がクリック可能になるまで待つ
    element = wait.until(
        EC.element_to_be_clickable((By.ID, 'button'))
    )
    
    # 要素が見えるようになるまで待つ
    element = wait.until(
        EC.visibility_of_element_located((By.CSS_SELECTOR, '.content'))
    )
    
except TimeoutException:
    # タイムアウトエラー（10秒経っても要素が見つからない）
    logger.error("要素が見つかりませんでした")


解説:

time.sleep()は固定時間待つだけで非効率

WebDriverWaitは条件が満たされた瞬間に次へ進むので効率的

タイムアウトも設定できる

例2: データをCSVに保存

import csv
from datetime import datetime

def save_to_csv(data_list, filename='output.csv'):
    """
    データをCSVファイルに保存
    Args:
        data_list: 辞書のリスト
        filename: 保存先ファイル名
    """
    # 現在時刻をファイル名に追加
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{filename.split('.')[0]}_{timestamp}.csv"
    
    # データが空の場合は終了
    if not data_list:
        logger.warning("保存するデータがありません")
        return
    
    # CSVファイルに書き込み
    with open(filename, 'w', encoding='utf-8', newline='') as f:
        # 最初の辞書のキーをヘッダーとして使用
        fieldnames = data_list[0].keys()
        
        # DictWriterオブジェクトを作成
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        
        # ヘッダー行を書き込み
        writer.writeheader()
        
        # データ行を1つずつ書き込み
        for data in data_list:
            writer.writerow(data)
    
    logger.info(f"CSVファイル保存: {filename} ({len(data_list)}件)")

# 使用例
scraped_data = [
    {'title': '記事1', 'url': 'https://example.com/1', 'date': '2025-10-04'},
    {'title': '記事2', 'url': 'https://example.com/2', 'date': '2025-10-05'},
]
save_to_csv(scraped_data, 'articles.csv')


例3: JSONに保存

import json
from datetime import datetime

def save_to_json(data, filename='output.json'):
    """
    データをJSONファイルに保存
    Args:
        data: 保存するデータ（辞書またはリスト）
        filename: 保存先ファイル名
    """
    # タイムスタンプ付きファイル名
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{filename.split('.')[0]}_{timestamp}.json"
    
    # JSONファイルに書き込み
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(
            data,
            f,
            ensure_ascii=False,  # 日本語をそのまま保存
            indent=2  # インデント（見やすさ）
        )
    
    logger.info(f"JSONファイル保存: {filename}")

# 使用例
data = {
    'url': 'https://example.com',
    'title': 'サンプルページ',
    'items': [
        {'name': 'アイテム1', 'price': 1000},
        {'name': 'アイテム2', 'price': 2000}
    ]
}
save_to_json(data, 'scraped_data.json')


例4: ページネーション対応

def scrape_with_pagination(base_url, max_pages=5):
    """
    複数ページをスクレイピング
    Args:
        base_url: ベースURL
        max_pages: 最大ページ数
    """
    # ドライバー設定
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # ヘッドレスモード
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    # 全結果を格納するリスト
    all_results = []
    
    try:
        # ページごとにループ
        for page_num in range(1, max_pages + 1):
            # ページ番号付きURLを生成
            url = f"{base_url}?page={page_num}"
            logger.info(f"ページ {page_num} をスクレイピング中: {url}")
            
            # ページにアクセス
            driver.get(url)
            time.sleep(2)  # 読み込み待機
            
            # HTML取得
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # 記事要素を取得
            items = soup.select('.item')
            
            # 要素が見つからない場合は終了
            if not items:
                logger.info(f"ページ {page_num} に要素が見つかりません。終了します。")
                break
            
            # 各記事を処理
            for item in items:
                # タイトルを取得
                title_elem = item.select_one('h2')
                title = title_elem.get_text(strip=True) if title_elem else ''
                
                # リンクを取得
                link_elem = item.select_one('a')
                link = link_elem.get('href', '') if link_elem else ''
                
                # データを格納
                result = {
                    'page': page_num,
                    'title': title,
                    'url': link
                }
                all_results.append(result)
            
            logger.info(f"ページ {page_num}: {len(items)}件取得")
            
            # 次のページボタンが存在するか確認
            next_button = soup.select_one('.pagination .next')
            if not next_button:
                logger.info("次のページが存在しません。終了します。")
                break
        
        logger.info(f"合計 {len(all_results)}件のデータを取得しました")
        return all_results
        
    except Exception as e:
        logger.error(f"エラー: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # ブラウザを閉じる
        driver.quit()
        logger.info("ブラウザを閉じました")

# 使用例
results = scrape_with_pagination("https://example.com/articles", max_pages=10)
save_to_csv(results, 'all_articles.csv')


解説:

range(1, max_pages + 1): 1からmax_pagesまでループ

ページごとにURLを生成してアクセス

次ページボタンの有無を確認して自動終了

例5: スクロール対応（無限スクロール）

def scrape_infinite_scroll(url, scroll_count=5):
    """
    無限スクロールページをスクレイピング
    Args:
        url: 対象URL
        scroll_count: スクロール回数
    """
    # ドライバー設定
    chrome_options = Options()
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    try:
        # ページにアクセス
        driver.get(url)
        time.sleep(3)
        
        # 現在のページ高さを取得
        last_height = driver.execute_script("return document.body.scrollHeight")
        
        # 指定回数スクロール
        for i in range(scroll_count):
            logger.info(f"スクロール {i + 1}/{scroll_count}")
            
            # 一番下までスクロール
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # 新しいコンテンツが読み込まれるまで待機
            time.sleep(2)
            
            # 新しいページ高さを取得
            new_height = driver.execute_script("return document.body.scrollHeight")
            
            # 高さが変わらなければ終了（これ以上コンテンツがない）
            if new_height == last_height:
                logger.info("これ以上スクロールできません")
                break
            
            # 高さを更新
            last_height = new_height
        
        # 全てのアイテムを取得
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        items = soup.select('.item')
        
        logger.info(f"合計 {len(items)}個のアイテムを取得しました")
        
        # データ抽出
        results = []
        for item in items:
            # データを抽出
            title = item.select_one('h2')
            title_text = title.get_text(strip=True) if title else ''
            
            results.append({'title': title_text})
        
        return results
        
    except Exception as e:
        logger.error(f"エラー: {e}")
        
    finally:
        driver.quit()

# 使用例
data = scrape_infinite_scroll("https://example.com", scroll_count=10)


解説:

execute_script(): JavaScriptを実行

scrollHeight: ページ全体の高さ

高さが変わらなくなったら終了

🛡️ エラーハンドリングのベストプラクティス

包括的なエラーハンドリング

from selenium.common.exceptions import (
    TimeoutException,
    NoSuchElementException,
    ElementNotInteractableException,
    WebDriverException
)

def safe_scrape(url):
    """
    エラーハンドリングを強化したスクレイピング
    """
    driver = None
    
    try:
        # ドライバーセットアップ
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # タイムアウト設定
        driver.set_page_load_timeout(30)  # ページ読み込みタイムアウト30秒
        
        # ページにアクセス
        try:
            driver.get(url)
        except TimeoutException:
            logger.error(f"ページ読み込みタイムアウト: {url}")
            return None
        
        # 要素を取得
        try:
            wait = WebDriverWait(driver, 10)
            element = wait.until(
                EC.presence_of_element_located((By.CLASS_NAME, 'item'))
            )
        except TimeoutException:
            logger.error("要素が見つかりませんでした")
            return None
        except NoSuchElementException:
            logger.error("指定された要素が存在しません")
            return None
        
        # データ抽出
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        items = soup.select('.item')
        
        # データが空の場合
        if not items:
            logger.warning("データが取得できませんでした")
            return []
        
        # 結果を返す
        return items
        
    except WebDriverException as e:
        # WebDriverに関するエラー
        logger.error(f"WebDriverエラー: {e}")
        return None
        
    except Exception as e:
        # その他のエラー
        logger.error(f"予期しないエラー: {e}")
        import traceback
        traceback.print_exc()
        return None
        
    finally:
        # 必ずドライバーを閉じる
        if driver:
            try:
                driver.quit()
                logger.info("ドライバーを正常に終了しました")
            except Exception as e:
                logger.error(f"ドライバー終了時にエラー: {e}")


解説:

各種エラーを個別にキャッチ

TimeoutException: タイムアウト

NoSuchElementException: 要素が見つからない

WebDriverException: WebDriver関連のエラー

finallyで必ずクリーンアップ

📝 ログレベルの使い分け

# DEBUG: 詳細なデバッグ情報（開発時のみ）
logger.debug(f"変数の値: {variable}")

# INFO: 通常の動作情報
logger.info("ページへのアクセスを開始")

# WARNING: 警告（処理は継続）
logger.warning("要素が見つかりませんでしたが、処理を続行します")

# ERROR: エラー（処理に影響）
logger.error("データの取得に失敗しました")

# CRITICAL: 致命的なエラー（プログラム停止レベル）
logger.critical("データベース接続に失敗しました")


🎯 スクレイピングのベストプラクティス

1. ロボット規約を確認

import requests

def check_robots_txt(base_url):
    """
    robots.txtを確認
    """
    robots_url = f"{base_url}/robots.txt"
    
    try:
        # robots.txtを取得
        response = requests.get(robots_url, timeout=5)
        
        # 成功した場合
        if response.status_code == 200:
            print(f"=== {base_url} の robots.txt ===")
            print(response.text)
            return response.text
        else:
            print(f"robots.txtが見つかりません（ステータス: {response.status_code}）")
            return None
            
    except Exception as e:
        print(f"robots.txt取得エラー: {e}")
        return None

# 使用例
check_robots_txt("https://example.com")


解説:

robots.txt: サイト運営者がクローラーに対して示すルール

スクレイピング前に必ず確認すべき

2. リクエスト間隔を空ける

import random

def polite_scraping(urls):
    """
    礼儀正しいスクレイピング
    """
    results = []
    
    for i, url in enumerate(urls, 1):
        logger.info(f"処理中 {i}/{len(urls)}: {url}")
        
        # データ取得
        data = scrape_page(url)
        results.append(data)
        
        # 最後のページ以外は待機
        if i < len(urls):
            # 1～3秒のランダムな待機時間
            wait_time = random.uniform(1.0, 3.0)
            logger.info(f"{wait_time:.1f}秒待機...")
            time.sleep(wait_time)
    
    return results


解説:

サーバーに負荷をかけないよう、リクエスト間隔を空ける

ランダムな待機時間で人間らしい動作を模倣

3. User-Agentの設定

# 良い例
chrome_options.add_argument(
    'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
)

# 悪い例（ボットとして識別される可能性）
# User-Agentを設定しない


🔍 デバッグテクニック

1. スクリーンショット撮影

def debug_screenshot(driver, filename_prefix='debug'):
    """
    デバッグ用スクリーンショット
    """
    # タイムスタンプ付きファイル名
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{filename_prefix}_{timestamp}.png"
    
    # スクリーンショット保存
    driver.save_screenshot(filename)
    logger.info(f"スクリーンショット保存: {filename}")
    
    return filename

# エラー時に使用
try:
    element = driver.find_element(By.CLASS_NAME, 'not-exist')
except NoSuchElementException:
    # エラー時の画面を保存
    debug_screenshot(driver, 'error')
    logger.error("要素が見つかりません")


2. HTML構造の出力

def print_element_structure(element, indent=0):
    """
    要素の構造を再帰的に表示
    Args:
        element: BeautifulSoup要素
        indent: インデントレベル
    """
    # インデント
    prefix = "  " * indent
    
    # タグ名とクラスを表示
    class_str = ' '.join(element.get('class', []))
    id_str = element.get('id', '')
    
    print(f"{prefix}<{element.name}>", end='')
    if id_str:
        print(f" id='{id_str}'", end='')
    if class_str:
        print(f" class='{class_str}'", end='')
    print()
    
    # 子要素を再帰的に表示
    for child in element.find_all(recursive=False):
        if child.name:  # テキストノードを除外
            print_element_structure(child, indent + 1)

# 使用例
soup = BeautifulSoup(html, 'html.parser')
body = soup.find('body')
print_element_structure(body)


📚 まとめ

このコードで学んだこと

ライブラリのインポート: 必要なモジュールを適切にインポート

ログ設定: プログラムの実行状況を記録

ドライバーセットアップ: Chromeを自動操作する準備

HTML解析: BeautifulSoupでDOM構造を解析

要素検索: CSSセレクターやタグ名で要素を探す

データ抽出: テキスト、属性、リンクなどを取得

エラーハンドリング: try-except-finallyで堅牢なコード

ファイル保存: HTMLやスクリーンショットを保存

次のステップ

✅ このコードを実行して動作を確認

✅ 別のサイトでスクレイピングを試す

✅ CSVやJSONへの保存機能を追加

✅ ページネーション対応を実装

✅ データベースへの保存を実装

🚀 実践課題

初級課題

# 課題1: h3タグを全て取得して表示する関数を作成
# 課題2: 画像のsrc属性を全て取得してリストに保存
# 課題3: 取得したデータをテキストファイルに保存


中級課題

# 課題4: 複数ページから記事タイトルを取得
# 課題5: カテゴリー別に記事を分類
# 課題6: スクレイピング結果をCSVに保存


上級課題

# 課題7: ログイン機能の実装
# 課題8: 動的コンテンツ（Ajax）の取得
# 課題9: データベース（SQLite）への保存


参考リンク

Selenium公式ドキュメント: https://www.selenium.dev/documentation/

BeautifulSoup公式ドキュメント: https://www.crummy.com/software/BeautifulSoup/bs4/doc/

Python公式ドキュメント: https://docs.python.org/ja/3/

お疲れ様でした！🎉 質問があればいつでも聞いてくださいね！

最終　動かせる全体コード（完全版）

# === 必要
# Seleniumの基本機能
from selenium import webdriver
# Chromeのオプション設定用
from selenium.webdriver.chrome.options import Options
# Chromeのサービス設定用
from selenium.webdriver.chrome.service import Service
# ChromeDriverを自動ダウンロードするツール
from webdriver_manager.chrome import ChromeDriverManager
# HTML要素を探すための方法を指定
from selenium.webdriver.common.by import By
# BeautifulSoupでHTML解析
from bs4 import BeautifulSoup
# 待機処理用
import time
# ログ出力用
import logging

# ログの設定
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def find_specific_elements(url):
    """
    特定のパターンの要素を探索する
    """
    # Chromeのオプションを作成
    chrome_options = Options()
    
    # ヘッドレスモードをオフ(画面を表示)
    # chrome_options.add_argument('--headless')  # コメントアウト=画面表示
    
    # その他の基本設定
    chrome_options.add_argument('--disable-gpu')  # GPU無効化
    chrome_options.add_argument('--no-sandbox')  # サンドボックス無効化
    chrome_options.add_argument('--window-size=1920,1080')  # ウィンドウサイズ
    
    # User-Agentを設定(ボット判定回避)
    chrome_options.add_argument(
        'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    )
    
    # ChromeDriverを自動インストール
    service = Service(ChromeDriverManager().install())
    
    # Chromeドライバーを起動
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    try:
        # 対象URLにアクセス
        logger.info(f"アクセス中: {url}")
        driver.get(url)
        
        # ページが完全に読み込まれるまで3秒待機
        time.sleep(3)
        
        # ページのHTMLソースを取得
        page_source = driver.page_source
        
        # BeautifulSoupで解析(HTMLを扱いやすくする)
        soup = BeautifulSoup(page_source, 'html.parser')
        
        print("\n" + "="*80)
        print("【coorikuya.com 詳細な要素検索】")
        print("="*80)
        
        # === パターン1: 記事一覧を探す ===
        print("\n1. 記事一覧候補:")
        # よくある記事一覧のパターン(CSSセレクター)
        patterns = [
            ('article'),  # articleタグ
            ('.post'),  # postクラス
            ('.entry'),  # entryクラス
            ('.blog-post'),  # blog-postクラス
            ('.article-item'),  # article-itemクラス
            ('[class*="post"]'),  # postを含むクラス
            ('.card'),  # cardクラス
            ('.item'),  # itemクラス
        ]
        
        # 各パターンで要素を検索
        for pattern in patterns:
            # CSSセレクターで要素を検索
            elements = soup.select(pattern)
            # 要素が見つかった場合
            if elements:
                print(f"  ✓ セレクター '{pattern}': {len(elements)}個見つかりました")
                # 最初の要素の構造を表示
                if elements:
                    first = elements[0]
                    # タグ名とクラス名を取得
                    class_list = first.get('class', [])
                    print(f"    構造例: <{first.name} class='{' '.join(class_list)}'>")
                    # 直接の子要素を確認
                    children = first.find_all(recursive=False)
                    print(f"    直接の子要素: {len(children)}個")
        
        # === パターン2: タイトルを探す ===
        print("\n2. タイトル候補:")
        title_patterns = [
            ('h1'),  # h1タグ
            ('h2'),  # h2タグ
            ('h2.entry-title'),  # entry-titleクラスを持つh2
            ('h2.post-title'),  # post-titleクラスを持つh2
            ('.title'),  # titleクラス
            ('article h2'),  # article内のh2
            ('h2 a'),  # リンクを含むh2
            ('.entry-title'),  # entry-titleクラス
        ]
        
        # 各パターンで検索
        for pattern in title_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  ✓ '{pattern}': {len(elements)}個")
                # 最初のテキストを表示(50文字まで)
                if elements:
                    text = elements[0].get_text(strip=True)
                    print(f"    例: {text[:50]}")
        
        # === パターン3: 日付を探す ===
        print("\n3. 日付候補:")
        date_patterns = [
            ('time'),  # timeタグ
            ('.date'),  # dateクラス
            ('.published'),  # publishedクラス
            ('.entry-date'),  # entry-dateクラス
            ('[datetime]'),  # datetime属性を持つ要素
            ('.post-date'),  # post-dateクラス
        ]
        
        # 各パターンで検索
        for pattern in date_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  ✓ '{pattern}': {len(elements)}個")
                if elements:
                    # テキストまたはdatetime属性を表示
                    text = elements[0].get_text(strip=True)
                    datetime_attr = elements[0].get('datetime', '')
                    print(f"    例: {text}")
                    if datetime_attr:
                        print(f"    datetime属性: {datetime_attr}")
        
        # === パターン4: 本文・抜粋を探す ===
        print("\n4. 本文・抜粋候補:")
        content_patterns = [
            ('.entry-content'),  # entry-contentクラス
            ('.post-content'),  # post-contentクラス
            ('.excerpt'),  # excerptクラス
            ('.summary'),  # summaryクラス
            ('article p'),  # article内のp(段落)
            ('.description'),  # descriptionクラス
        ]
        
        # 各パターンで検索
        for pattern in content_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  ✓ '{pattern}': {len(elements)}個")
                if elements:
                    # テキストを取得(最初の80文字)
                    text = elements[0].get_text(strip=True)
                    print(f"    例: {text[:80]}...")
        
        # === パターン5: カテゴリー・タグを探す ===
        print("\n5. カテゴリー・タグ候補:")
        category_patterns = [
            ('.category'),  # categoryクラス
            ('.tag'),  # tagクラス
            ('.categories'),  # categoriesクラス
            ('.tags'),  # tagsクラス
            ('a[rel="category"]'),  # rel属性がcategoryのリンク
            ('a[rel="tag"]'),  # rel属性がtagのリンク
            ('.cat-links'),  # cat-linksクラス
        ]
        
        # 各パターンで検索
        for pattern in category_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  ✓ '{pattern}': {len(elements)}個")
                if elements:
                    # テキストを表示
                    print(f"    例: {elements[0].get_text(strip=True)}")
        
        # === パターン6: 画像を探す ===
        print("\n6. 画像候補:")
        images = soup.find_all('img')
        print(f"  総画像数: {len(images)}個")
        
        # 最初の3つの画像を表示
        for i, img in enumerate(images[:3], 1):
            src = img.get('src', '')
            alt = img.get('alt', '')
            print(f"  {i}. alt='{alt[:30]}' src='{src[:60]}'")
        
        # === パターン7: リンクを探す ===
        print("\n7. リンク候補:")
        links = soup.find_all('a', href=True)
        print(f"  総リンク数: {len(links)}個")
        
        # 最初の5つのリンクを表示
        for i, link in enumerate(links[:5], 1):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            print(f"  {i}. {text[:30]} -> {href[:50]}")
        
        # === 全体のHTML構造ツリーを表示 ===
        print("\n8. HTML構造ツリー(body直下の要素):")
        body = soup.find('body')
        if body:
            # bodyの直接の子要素を列挙
            for i, child in enumerate(body.find_all(recursive=False), 1):
                # クラス名を取得
                class_str = ' '.join(child.get('class', []))
                # ID属性を取得
                id_str = child.get('id', '')
                
                # 出力
                print(f"  {i}. <{child.name}> ", end='')
                if id_str:
                    print(f"id='{id_str}' ", end='')
                if class_str:
                    print(f"class='{class_str}'", end='')
                print()
        
        # === よく使われているクラス名のランキング ===
        print("\n9. 頻出クラス名トップ10:")
        class_counter = {}
        # 全てのタグからクラス名を収集
        for tag in soup.find_all(class_=True):
            classes = tag.get('class', [])
            for cls in classes:
                # カウント
                class_counter[cls] = class_counter.get(cls, 0) + 1
        
        # 出現回数でソート
        sorted_classes = sorted(
            class_counter.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # トップ10を表示
        for i, (cls, count) in enumerate(sorted_classes[:10], 1):
            print(f"  {i:2d}. '{cls}' - {count}回")
        
        # === HTMLソースを保存 ===
        print("\n10. HTMLソース保存:")
        with open('coorikuya_source.html', 'w', encoding='utf-8') as f:
            # きれいに整形して保存
            f.write(soup.prettify())
        print("  ✓ 'coorikuya_source.html' に保存しました")
        
        # === スクリーンショット保存 ===
        driver.save_screenshot('coorikuya_screenshot.png')
        print("  ✓ 'coorikuya_screenshot.png' に保存しました")
        
        print("\n" + "="*80)
        print("【分析完了！】")
        print("="*80)
        
    except Exception as e:
        # エラーが発生した場合
        logger.error(f"エラーが発生しました: {e}")
        # 詳細なエラー情報を表示
        import traceback
        traceback.print_exc()
        
    finally:
        # 必ずブラウザを閉じる
        print("\n5秒後にブラウザを閉じます...")
        time.sleep(5)
        driver.quit()
        logger.info("ブラウザを閉じました")


# このファイルが直接実行された場合のみ実行
if __name__ == "__main__":
    # coorikuya.comを分析
    find_specific_elements("https://www.coorikuya.com/")


実行方法

# ファイルを保存(例: analyze_coorikuya.py)
# 実行
py analyze_coorikuya.py


修正したポイント

✅ 全てのインポート文を追加

from selenium import webdriver from selenium.webdriver.chrome.options import Options from selenium.webdriver.chrome.service import Service from webdriver_manager.chrome import ChromeDriverManager from bs4 import BeautifulSoup import time import logging

✅ ログ設定を追加

✅ より詳細な分析項目を追加

画像分析

リンク分析

クラス名ランキング

✅ 結果を保存

HTMLソース → coorikuya_source.html

スクリーンショット → coorikuya_screenshot.png

実行すると得られる情報

✨ このコードを実行すると:

記事一覧の構造が分かる

タイトルの取得方法が分かる

日付の取得方法が分かる

本文の取得方法が分かる

カテゴリーの取得方法が分かる

HTML全体が保存される

スクリーンショットが保存される

これを実行してみて、結果を教えてください！ 次は実際のスクレイピングコードを作りますよ！🐍✨
