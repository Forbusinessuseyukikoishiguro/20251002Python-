ã¯ã˜ã‚ã«

ã“ã®è¨˜äº‹ã§ã¯ã€Pythonã®Seleniumã¨BeautifulSoupã‚’ä½¿ã£ãŸWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®åŸºç¤ã‚’ã€ã‚³ãƒ¼ãƒ‰1è¡Œãšã¤ä¸å¯§ã«è§£èª¬ã—ã¾ã™ã€‚å®Ÿéš›ã«å‹•ãã‚³ãƒ¼ãƒ‰ã‚’é€šã˜ã¦ã€Webãƒšãƒ¼ã‚¸ã®æ§‹é€ åˆ†æã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã¾ã§ã‚’å­¦ã³ã¾ã—ã‚‡ã†ï¼

1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

ã‚³ãƒ¼ãƒ‰å…¨ä½“ï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆéƒ¨åˆ†ï¼‰

# === å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å…¨ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ===
# Seleniumã®åŸºæœ¬æ©Ÿèƒ½
from selenium import webdriver
# Chromeã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šç”¨
from selenium.webdriver.chrome.options import Options
# Chromeã®ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šç”¨
from selenium.webdriver.chrome.service import Service
# ChromeDriverã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ„ãƒ¼ãƒ«
from webdriver_manager.chrome import ChromeDriverManager
# HTMLè¦ç´ ã‚’æ¢ã™ãŸã‚ã®æ–¹æ³•ã‚’æŒ‡å®š
from selenium.webdriver.common.by import By
# BeautifulSoupã§HTMLè§£æ
from bs4 import BeautifulSoup
# å¾…æ©Ÿå‡¦ç†ç”¨
import time
# ãƒ­ã‚°å‡ºåŠ›ç”¨
import logging


1è¡Œãšã¤è§£èª¬

# Seleniumã®åŸºæœ¬æ©Ÿèƒ½
from selenium import webdriver


è§£èª¬: Seleniumã®ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã‚’è‡ªå‹•æ“ä½œã™ã‚‹ãŸã‚ã®åŸºæœ¬æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

# Chromeã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šç”¨
from selenium.webdriver.chrome.options import Options


è§£èª¬: Chromeãƒ–ãƒ©ã‚¦ã‚¶ã®å‹•ä½œè¨­å®šï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãªã©ï¼‰ã‚’æŒ‡å®šã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

# Chromeã®ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šç”¨
from selenium.webdriver.chrome.service import Service


è§£èª¬: ChromeDriverã®ãƒ‘ã‚¹ã‚„èµ·å‹•ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

# ChromeDriverã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ„ãƒ¼ãƒ«
from webdriver_manager.chrome import ChromeDriverManager


è§£èª¬: ChromeDriverã‚’æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã›ãšã€è‡ªå‹•çš„ã«é©åˆ‡ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—ãƒ»ç®¡ç†ã—ã¦ãã‚Œã‚‹ä¾¿åˆ©ãªãƒ„ãƒ¼ãƒ«ã€‚

# HTMLè¦ç´ ã‚’æ¢ã™ãŸã‚ã®æ–¹æ³•ã‚’æŒ‡å®š
from selenium.webdriver.common.by import By


è§£èª¬: HTMLè¦ç´ ã‚’æ¤œç´¢ã™ã‚‹æ–¹æ³•ï¼ˆIDã€ã‚¯ãƒ©ã‚¹åã€CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ãªã©ï¼‰ã‚’æŒ‡å®šã™ã‚‹ãŸã‚ã®å®šæ•°ã‚’æä¾›ã€‚

# BeautifulSoupã§HTMLè§£æ
from bs4 import BeautifulSoup


è§£èª¬: HTMLã‚’è§£æã—ã¦ã€è¦ç´ ã®æŠ½å‡ºã‚„æ“ä½œã‚’ç°¡å˜ã«ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚Seleniumã§å–å¾—ã—ãŸHTMLã‚’æ•´å½¢ãƒ»æ¤œç´¢ã™ã‚‹ã®ã«ä½¿ç”¨ã€‚

# å¾…æ©Ÿå‡¦ç†ç”¨
import time


è§£èª¬: ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä¸€æ™‚åœæ­¢ã•ã›ã‚‹ãŸã‚ã®æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿãªã©ã«ä½¿ç”¨ã€‚

# ãƒ­ã‚°å‡ºåŠ›ç”¨
import logging


è§£èª¬: ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡ŒçŠ¶æ³ã‚„ ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã®æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚ãƒ‡ãƒãƒƒã‚°ã‚„ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«å¿…é ˆã€‚

2. ãƒ­ã‚°è¨­å®šã®é‡è¦æ€§

# ãƒ­ã‚°ã®è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


1è¡Œãšã¤è§£èª¬

logging.basicConfig(


è§£èª¬: ãƒ­ã‚°ã®åŸºæœ¬è¨­å®šã‚’è¡Œã†é–¢æ•°ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ å…¨ä½“ã®ãƒ­ã‚°å‹•ä½œã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚

    level=logging.INFO,


è§£èª¬: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’INFOã«è¨­å®šã€‚DEBUGï¼ˆè©³ç´°ï¼‰ã€INFOï¼ˆé€šå¸¸ï¼‰ã€WARNINGï¼ˆè­¦å‘Šï¼‰ã€ERRORï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰ã€CRITICALï¼ˆè‡´å‘½çš„ï¼‰ã®5æ®µéšãŒã‚ã‚Šã€INFOä»¥ä¸Šã®ãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

    format='%(asctime)s - %(levelname)s - %(message)s'


è§£èª¬: ãƒ­ã‚°ã®å‡ºåŠ›å½¢å¼ã‚’æŒ‡å®šã€‚

%(asctime)s: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆä¾‹: 2025-10-04 15:05:23,886ï¼‰

%(levelname)s: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ï¼ˆINFOã€ERROR ãªã©ï¼‰

%(message)s: ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æœ¬æ–‡

logger = logging.getLogger(__name__)


è§£èª¬: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨ã®ãƒ­ã‚¬ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã€‚__name__ã¯ç¾åœ¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åãŒå…¥ã‚Šã¾ã™ã€‚

3. ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

def find_specific_elements(url):
    """
    ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦ç´ ã‚’æ¢ç´¢ã™ã‚‹
    """


è§£èª¬

def find_specific_elements(url):


è§£èª¬: é–¢æ•°å®šç¾©ã€‚å¼•æ•°urlã«ã¯åˆ†æã—ãŸã„Webãƒšãƒ¼ã‚¸ã®URLã‚’æ¸¡ã—ã¾ã™ã€‚

    """
    ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦ç´ ã‚’æ¢ç´¢ã™ã‚‹
    """


è§£èª¬: Docstringï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—ï¼‰ã€‚é–¢æ•°ã®èª¬æ˜ã‚’è¨˜è¿°ã—ã¾ã™ã€‚ä»–ã®é–‹ç™ºè€…ã‚„å°†æ¥ã®è‡ªåˆ†ãŒè¦‹ã¦ã‚‚ã‚ã‹ã‚‹ã‚ˆã†ã«æ›¸ãã¾ã™ã€‚

Chromeã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è¨­å®š

    # Chromeã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    chrome_options = Options()


è§£èª¬: Chromeãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã‚’æ ¼ç´ã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã€‚ã“ã®å¾Œã€æ§˜ã€…ãªè¨­å®šã‚’è¿½åŠ ã—ã¦ã„ãã¾ã™ã€‚

    # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ•(ç”»é¢ã‚’è¡¨ç¤º)
    # chrome_options.add_argument('--headless')  # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ=ç”»é¢è¡¨ç¤º


è§£èª¬:

--headless: ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ç”»é¢ã«è¡¨ç¤ºã›ãšã«å®Ÿè¡Œã™ã‚‹ãƒ¢ãƒ¼ãƒ‰

ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ç¾åœ¨ã¯ç”»é¢ãŒè¡¨ç¤ºã•ã‚Œã‚‹è¨­å®š

ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã™ã¨ç”»é¢éè¡¨ç¤ºã§é«˜é€Ÿå®Ÿè¡Œã§ãã¾ã™

    # ãã®ä»–ã®åŸºæœ¬è¨­å®š
    chrome_options.add_argument('--disable-gpu')  # GPUç„¡åŠ¹åŒ–


è§£èª¬: GPUï¼ˆã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯å‡¦ç†ï¼‰ã‚’ç„¡åŠ¹åŒ–ã€‚ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã‚’å®‰å®šã•ã›ã‚‹ãŸã‚ã€‚

    chrome_options.add_argument('--no-sandbox')  # ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ç„¡åŠ¹åŒ–


è§£èª¬: ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¿è­·æ©Ÿèƒ½ï¼‰ã‚’ç„¡åŠ¹åŒ–ã€‚Linuxã‚„ã‚³ãƒ³ãƒ†ãƒŠç’°å¢ƒã§å¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™ã€‚

    chrome_options.add_argument('--window-size=1920,1080')  # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º


è§£èª¬: ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’1920x1080ãƒ”ã‚¯ã‚»ãƒ«ã«è¨­å®šã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³ã®ã‚µã‚¤ãƒˆã§é‡è¦ã€‚

    # User-Agentã‚’è¨­å®š(ãƒœãƒƒãƒˆåˆ¤å®šå›é¿)
    chrome_options.add_argument(
        'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    )


è§£èª¬: User-Agentï¼ˆãƒ–ãƒ©ã‚¦ã‚¶è­˜åˆ¥æƒ…å ±ï¼‰ã‚’è¨­å®šã€‚é€šå¸¸ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¨ã—ã¦èªè­˜ã•ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ã€ãƒœãƒƒãƒˆåˆ¤å®šã‚’å›é¿ã—ã¾ã™ã€‚

ChromeDriverã®èµ·å‹•

    # ChromeDriverã‚’è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    service = Service(ChromeDriverManager().install())


è§£èª¬:

ChromeDriverManager().install(): ChromeDriverã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

Service(): ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ

    # Chromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’èµ·å‹•
    driver = webdriver.Chrome(service=service, options=chrome_options)


è§£èª¬:

webdriver.Chrome(): Chromeãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•

service: å…ˆã»ã©ä½œæˆã—ãŸã‚µãƒ¼ãƒ“ã‚¹ï¼ˆãƒ‰ãƒ©ã‚¤ãƒãƒ¼æƒ…å ±ï¼‰ã‚’æ¸¡ã™

options: è¨­å®šã—ãŸã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’é©ç”¨

4. ãƒšãƒ¼ã‚¸ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã¨HTMLå–å¾—

    try:


è§£èª¬: ä¾‹å¤–å‡¦ç†ã®é–‹å§‹ã€‚ã“ã®ä¸­ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€exceptãƒ–ãƒ­ãƒƒã‚¯ã§å‡¦ç†ã•ã‚Œã¾ã™ã€‚

        # å¯¾è±¡URLã«ã‚¢ã‚¯ã‚»ã‚¹
        logger.info(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")


è§£èª¬: INFOãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’å‡ºåŠ›ã€‚f"..." ã¯f-stringï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿æ–‡å­—åˆ—ï¼‰ã§ã€å¤‰æ•°ã‚’åŸ‹ã‚è¾¼ã‚ã¾ã™ã€‚

        driver.get(url)


è§£èª¬: æŒ‡å®šã•ã‚ŒãŸURLã«ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹ã€‚ã“ã®æ™‚ç‚¹ã§ãƒšãƒ¼ã‚¸ãŒé–‹ã‹ã‚Œã¾ã™ã€‚

        # ãƒšãƒ¼ã‚¸ãŒå®Œå…¨ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§3ç§’å¾…æ©Ÿ
        time.sleep(3)


è§£èª¬: 3ç§’é–“ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’åœæ­¢ã€‚JavaScriptã§å‹•çš„ã«ç”Ÿæˆã•ã‚Œã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿ã‚’å¾…ã¡ã¾ã™ã€‚

        # ãƒšãƒ¼ã‚¸ã®HTMLã‚½ãƒ¼ã‚¹ã‚’å–å¾—
        page_source = driver.page_source


è§£èª¬: ç¾åœ¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã®å®Œå…¨ãªHTMLã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’æ–‡å­—åˆ—ã¨ã—ã¦å–å¾—ã€‚

        # BeautifulSoupã§è§£æ(HTMLã‚’æ‰±ã„ã‚„ã™ãã™ã‚‹)
        soup = BeautifulSoup(page_source, 'html.parser')


è§£èª¬:

BeautifulSoup(): HTMLã‚’è§£æã—ã¦ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåŒ–

page_source: è§£æã™ã‚‹HTMLæ–‡å­—åˆ—

'html.parser': ä½¿ç”¨ã™ã‚‹ãƒ‘ãƒ¼ã‚µãƒ¼ï¼ˆè§£æã‚¨ãƒ³ã‚¸ãƒ³ï¼‰

5. HTMLè¦ç´ ã®æ¤œç´¢

        print("\n" + "="*80)
        print("ã€coorikuya.com è©³ç´°ãªè¦ç´ æ¤œç´¢ã€‘")
        print("="*80)


è§£èª¬:

"\n": æ”¹è¡Œ

"="*80: =ã‚’80å›ç¹°ã‚Šè¿”ã—ã¦åŒºåˆ‡ã‚Šç·šã‚’ä½œæˆ

è¦‹ã‚„ã™ã„å‡ºåŠ›ã®ãŸã‚ã®è£…é£¾

ãƒ‘ã‚¿ãƒ¼ãƒ³1: è¨˜äº‹ä¸€è¦§ã‚’æ¢ã™

        # === ãƒ‘ã‚¿ãƒ¼ãƒ³1: è¨˜äº‹ä¸€è¦§ã‚’æ¢ã™ ===
        print("\n1. è¨˜äº‹ä¸€è¦§å€™è£œ:")


è§£èª¬: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é–‹å§‹ã‚’è¡¨ç¤ºã€‚

        # ã‚ˆãã‚ã‚‹è¨˜äº‹ä¸€è¦§ã®ãƒ‘ã‚¿ãƒ¼ãƒ³(CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼)
        patterns = [
            ('article'),  # articleã‚¿ã‚°
            ('.post'),  # postã‚¯ãƒ©ã‚¹
            ('.entry'),  # entryã‚¯ãƒ©ã‚¹
            ('.blog-post'),  # blog-postã‚¯ãƒ©ã‚¹
            ('.article-item'),  # article-itemã‚¯ãƒ©ã‚¹
            ('[class*="post"]'),  # postã‚’å«ã‚€ã‚¯ãƒ©ã‚¹
            ('.card'),  # cardã‚¯ãƒ©ã‚¹
            ('.item'),  # itemã‚¯ãƒ©ã‚¹
        ]


è§£èª¬:

ãƒªã‚¹ãƒˆå‹ã®å¤‰æ•°ã‚’å®šç¾©

CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼å½¢å¼ã§æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ—æŒ™

article: ã‚¿ã‚°åã§æ¤œç´¢

.post: ã‚¯ãƒ©ã‚¹åã§æ¤œç´¢ï¼ˆãƒ‰ãƒƒãƒˆä»˜ãï¼‰

[class*="post"]: éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ï¼ˆpostã‚’å«ã‚€ã‚¯ãƒ©ã‚¹ï¼‰

        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§è¦ç´ ã‚’æ¤œç´¢
        for pattern in patterns:


è§£èª¬: patternsãƒªã‚¹ãƒˆã®å„è¦ç´ ã‚’é †ç•ªã«patternå¤‰æ•°ã«æ ¼ç´ã—ã¦ãƒ«ãƒ¼ãƒ—å‡¦ç†ã€‚

            # CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§è¦ç´ ã‚’æ¤œç´¢
            elements = soup.select(pattern)


è§£èª¬:

soup.select(): CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§è¦ç´ ã‚’æ¤œç´¢

è©²å½“ã™ã‚‹å…¨ã¦ã®è¦ç´ ã‚’ãƒªã‚¹ãƒˆã§è¿”ã™

è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆ[]

            # è¦ç´ ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
            if elements:


è§£èª¬: ãƒªã‚¹ãƒˆãŒç©ºã§ãªã„å ´åˆï¼ˆè¦ç´ ãŒ1ã¤ä»¥ä¸Šè¦‹ã¤ã‹ã£ãŸå ´åˆï¼‰ã«å‡¦ç†ã‚’å®Ÿè¡Œã€‚

                print(f"  âœ“ ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ '{pattern}': {len(elements)}å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")


è§£èª¬:

len(elements): ãƒªã‚¹ãƒˆã®è¦ç´ æ•°ã‚’å–å¾—

è¦‹ã¤ã‹ã£ãŸè¦ç´ ã®æ•°ã‚’è¡¨ç¤º

                # æœ€åˆã®è¦ç´ ã®æ§‹é€ ã‚’è¡¨ç¤º
                if elements:
                    first = elements[0]


è§£èª¬:

elements[0]: ãƒªã‚¹ãƒˆã®æœ€åˆã®è¦ç´ ã‚’å–å¾—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯0ã‹ã‚‰å§‹ã¾ã‚‹ï¼‰

firstå¤‰æ•°ã«æ ¼ç´

                    # ã‚¿ã‚°åã¨ã‚¯ãƒ©ã‚¹åã‚’å–å¾—
                    class_list = first.get('class', [])


è§£èª¬:

first.get('class', []): è¦ç´ ã®classå±æ€§ã‚’å–å¾—

å­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆ[]ã‚’è¿”ã™

                    print(f"    æ§‹é€ ä¾‹: <{first.name} class='{' '.join(class_list)}'>")


è§£èª¬:

first.name: ã‚¿ã‚°åï¼ˆä¾‹: div, articleï¼‰

' '.join(class_list): ãƒªã‚¹ãƒˆã‚’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã«å¤‰æ›

ä¾‹: ['item', 'active'] â†’ 'item active'

                    # ç›´æ¥ã®å­è¦ç´ ã‚’ç¢ºèª
                    children = first.find_all(recursive=False)


è§£èª¬:

find_all(): å…¨ã¦ã®å­å­«è¦ç´ ã‚’æ¤œç´¢

recursive=False: ç›´æ¥ã®å­è¦ç´ ã®ã¿ï¼ˆå­«è¦ç´ ã¯å«ã¾ãªã„ï¼‰

                    print(f"    ç›´æ¥ã®å­è¦ç´ : {len(children)}å€‹")


è§£èª¬: å­è¦ç´ ã®æ•°ã‚’è¡¨ç¤ºã€‚

ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™

        # === ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™ ===
        print("\n2. ã‚¿ã‚¤ãƒˆãƒ«å€™è£œ:")
        title_patterns = [
            ('h1'),  # h1ã‚¿ã‚°
            ('h2'),  # h2ã‚¿ã‚°
            ('h2.entry-title'),  # entry-titleã‚¯ãƒ©ã‚¹ã‚’æŒã¤h2
            ('h2.post-title'),  # post-titleã‚¯ãƒ©ã‚¹ã‚’æŒã¤h2
            ('.title'),  # titleã‚¯ãƒ©ã‚¹
            ('article h2'),  # articleå†…ã®h2
            ('h2 a'),  # ãƒªãƒ³ã‚¯ã‚’å«ã‚€h2
            ('.entry-title'),  # entry-titleã‚¯ãƒ©ã‚¹
        ]


è§£èª¬:

ã‚¿ã‚¤ãƒˆãƒ«è¦ç´ ã‚’æ¢ã™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒªã‚¹ãƒˆ

h2.entry-title: ã‚¿ã‚°ã¨ã‚¯ãƒ©ã‚¹ã®çµ„ã¿åˆã‚ã›

article h2: å­å­«ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼ˆarticleå†…ã®h2ï¼‰

h2 a: h2å†…ã®aã‚¿ã‚°

        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        for pattern in title_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  âœ“ '{pattern}': {len(elements)}å€‹")


è§£èª¬: ã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ«ãƒ¼ãƒ—ã—ã¦æ¤œç´¢çµæœã‚’è¡¨ç¤ºã€‚

                # æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º(50æ–‡å­—ã¾ã§)
                if elements:
                    text = elements[0].get_text(strip=True)


è§£èª¬:

get_text(): è¦ç´ å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆHTMLã‚¿ã‚°ã‚’é™¤å»ï¼‰

strip=True: å‰å¾Œã®ç©ºç™½ãƒ»æ”¹è¡Œã‚’å‰Šé™¤

                    print(f"    ä¾‹: {text[:50]}")


è§£èª¬:

text[:50]: æ–‡å­—åˆ—ã®æœ€åˆã®50æ–‡å­—ã‚’å–å¾—ï¼ˆã‚¹ãƒ©ã‚¤ã‚¹ï¼‰

é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯çœç•¥è¡¨ç¤º

ãƒ‘ã‚¿ãƒ¼ãƒ³3: æ—¥ä»˜ã‚’æ¢ã™

        # === ãƒ‘ã‚¿ãƒ¼ãƒ³3: æ—¥ä»˜ã‚’æ¢ã™ ===
        print("\n3. æ—¥ä»˜å€™è£œ:")
        date_patterns = [
            ('time'),  # timeã‚¿ã‚°
            ('.date'),  # dateã‚¯ãƒ©ã‚¹
            ('.published'),  # publishedã‚¯ãƒ©ã‚¹
            ('.entry-date'),  # entry-dateã‚¯ãƒ©ã‚¹
            ('[datetime]'),  # datetimeå±æ€§ã‚’æŒã¤è¦ç´ 
            ('.post-date'),  # post-dateã‚¯ãƒ©ã‚¹
        ]


è§£èª¬:

æ—¥ä»˜è¦ç´ ã‚’æ¢ã™ãƒ‘ã‚¿ãƒ¼ãƒ³

[datetime]: å±æ€§ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼ˆdatetimeå±æ€§ã‚’æŒã¤è¦ç´ ï¼‰

        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        for pattern in date_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  âœ“ '{pattern}': {len(elements)}å€‹")
                if elements:
                    # ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯datetimeå±æ€§ã‚’è¡¨ç¤º
                    text = elements[0].get_text(strip=True)
                    datetime_attr = elements[0].get('datetime', '')


è§£èª¬:

ãƒ†ã‚­ã‚¹ãƒˆã¨datetimeå±æ€§ã®ä¸¡æ–¹ã‚’å–å¾—

datetimeå±æ€§ã¯æ©Ÿæ¢°å¯èª­ãªæ—¥æ™‚å½¢å¼ï¼ˆä¾‹: 2025-10-04T15:05:23ï¼‰

                    print(f"    ä¾‹: {text}")
                    if datetime_attr:
                        print(f"    datetimeå±æ€§: {datetime_attr}")


è§£èª¬: datetimeå±æ€§ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç¤ºã€‚

ãƒ‘ã‚¿ãƒ¼ãƒ³4: æœ¬æ–‡ãƒ»æŠœç²‹ã‚’æ¢ã™

        # === ãƒ‘ã‚¿ãƒ¼ãƒ³4: æœ¬æ–‡ãƒ»æŠœç²‹ã‚’æ¢ã™ ===
        print("\n4. æœ¬æ–‡ãƒ»æŠœç²‹å€™è£œ:")
        content_patterns = [
            ('.entry-content'),  # entry-contentã‚¯ãƒ©ã‚¹
            ('.post-content'),  # post-contentã‚¯ãƒ©ã‚¹
            ('.excerpt'),  # excerptã‚¯ãƒ©ã‚¹
            ('.summary'),  # summaryã‚¯ãƒ©ã‚¹
            ('article p'),  # articleå†…ã®p(æ®µè½)
            ('.description'),  # descriptionã‚¯ãƒ©ã‚¹
        ]


è§£èª¬: æœ¬æ–‡ã‚„è¦ç´„ã‚’å«ã‚€è¦ç´ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‚

        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        for pattern in content_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  âœ“ '{pattern}': {len(elements)}å€‹")
                if elements:
                    # ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—(æœ€åˆã®80æ–‡å­—)
                    text = elements[0].get_text(strip=True)
                    print(f"    ä¾‹: {text[:80]}...")


è§£èª¬: æœ¬æ–‡ã®æœ€åˆã®80æ–‡å­—ã‚’è¡¨ç¤ºã€‚...ã§çœç•¥ã‚’ç¤ºã—ã¾ã™ã€‚

ãƒ‘ã‚¿ãƒ¼ãƒ³5: ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ»ã‚¿ã‚°ã‚’æ¢ã™

        # === ãƒ‘ã‚¿ãƒ¼ãƒ³5: ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ»ã‚¿ã‚°ã‚’æ¢ã™ ===
        print("\n5. ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ»ã‚¿ã‚°å€™è£œ:")
        category_patterns = [
            ('.category'),  # categoryã‚¯ãƒ©ã‚¹
            ('.tag'),  # tagã‚¯ãƒ©ã‚¹
            ('.categories'),  # categoriesã‚¯ãƒ©ã‚¹
            ('.tags'),  # tagsã‚¯ãƒ©ã‚¹
            ('a[rel="category"]'),  # relå±æ€§ãŒcategoryã®ãƒªãƒ³ã‚¯
            ('a[rel="tag"]'),  # relå±æ€§ãŒtagã®ãƒªãƒ³ã‚¯
            ('.cat-links'),  # cat-linksã‚¯ãƒ©ã‚¹
        ]


è§£èª¬:

ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚„ã‚¿ã‚°ã‚’æ¢ã™ãƒ‘ã‚¿ãƒ¼ãƒ³

a[rel="category"]: å±æ€§å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        for pattern in category_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  âœ“ '{pattern}': {len(elements)}å€‹")
                if elements:
                    # ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
                    print(f"    ä¾‹: {elements[0].get_text(strip=True)}")


è§£èª¬: ã‚«ãƒ†ã‚´ãƒªãƒ¼åã‚’è¡¨ç¤ºã€‚

ãƒ‘ã‚¿ãƒ¼ãƒ³6: ç”»åƒã‚’æ¢ã™

        # === ãƒ‘ã‚¿ãƒ¼ãƒ³6: ç”»åƒã‚’æ¢ã™ ===
        print("\n6. ç”»åƒå€™è£œ:")
        images = soup.find_all('img')


è§£èª¬:

find_all('img'): å…¨ã¦ã®<img>ã‚¿ã‚°ã‚’æ¤œç´¢

CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ã¯ãªãã‚¿ã‚°åã§ç›´æ¥æ¤œç´¢

        print(f"  ç·ç”»åƒæ•°: {len(images)}å€‹")


è§£èª¬: ç”»åƒã®ç·æ•°ã‚’è¡¨ç¤ºã€‚

        # æœ€åˆã®3ã¤ã®ç”»åƒã‚’è¡¨ç¤º
        for i, img in enumerate(images[:3], 1):


è§£èª¬:

images[:3]: æœ€åˆã®3ã¤ã®è¦ç´ 

enumerate(..., 1): ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨è¦ç´ ã‚’åŒæ™‚ã«å–å¾—ã€ç•ªå·ã¯1ã‹ã‚‰é–‹å§‹

            src = img.get('src', '')
            alt = img.get('alt', '')


è§£èª¬:

src: ç”»åƒã®URL

alt: ä»£æ›¿ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç”»åƒãŒè¡¨ç¤ºã•ã‚Œãªã„æ™‚ã«è¡¨ç¤ºã•ã‚Œã‚‹èª¬æ˜æ–‡ï¼‰

            print(f"  {i}. alt='{alt[:30]}' src='{src[:60]}'")


è§£èª¬: ç”»åƒæƒ…å ±ã‚’è¡¨ç¤ºã€‚é•·ã„å ´åˆã¯çœç•¥ã€‚

ãƒ‘ã‚¿ãƒ¼ãƒ³7: ãƒªãƒ³ã‚¯ã‚’æ¢ã™

        # === ãƒ‘ã‚¿ãƒ¼ãƒ³7: ãƒªãƒ³ã‚¯ã‚’æ¢ã™ ===
        print("\n7. ãƒªãƒ³ã‚¯å€™è£œ:")
        links = soup.find_all('a', href=True)


è§£èª¬:

find_all('a', href=True): hrefå±æ€§ã‚’æŒã¤<a>ã‚¿ã‚°ã®ã¿æ¤œç´¢

hrefãŒãªã„ãƒªãƒ³ã‚¯ã¯é™¤å¤–ã•ã‚Œã‚‹

        print(f"  ç·ãƒªãƒ³ã‚¯æ•°: {len(links)}å€‹")
        
        # æœ€åˆã®5ã¤ã®ãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º
        for i, link in enumerate(links[:5], 1):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            print(f"  {i}. {text[:30]} -> {href[:50]}")


è§£èª¬: ãƒªãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆã¨URLã‚’è¡¨ç¤ºã€‚

6. HTMLæ§‹é€ ãƒ„ãƒªãƒ¼ã®è¡¨ç¤º

        # === å…¨ä½“ã®HTMLæ§‹é€ ãƒ„ãƒªãƒ¼ã‚’è¡¨ç¤º ===
        print("\n8. HTMLæ§‹é€ ãƒ„ãƒªãƒ¼(bodyç›´ä¸‹ã®è¦ç´ ):")
        body = soup.find('body')


è§£èª¬:

find('body'): æœ€åˆã®<body>ã‚¿ã‚°ã‚’æ¤œç´¢

find_all()ã¨ç•°ãªã‚Šã€1ã¤ã ã‘è¿”ã™

        if body:
            # bodyã®ç›´æ¥ã®å­è¦ç´ ã‚’åˆ—æŒ™
            for i, child in enumerate(body.find_all(recursive=False), 1):


è§£èª¬:

bodyãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†

recursive=False: ç›´æ¥ã®å­è¦ç´ ã®ã¿

                # ã‚¯ãƒ©ã‚¹åã‚’å–å¾—
                class_str = ' '.join(child.get('class', []))
                # IDå±æ€§ã‚’å–å¾—
                id_str = child.get('id', '')


è§£èª¬: ã‚¯ãƒ©ã‚¹åã¨IDå±æ€§ã‚’å–å¾—ã€‚

                # å‡ºåŠ›
                print(f"  {i}. <{child.name}> ", end='')


è§£èª¬:

end='': æ”¹è¡Œã›ãšã«æ¬¡ã®å‡ºåŠ›ã‚’ç¶šã‘ã‚‹

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯end='\n'ï¼ˆæ”¹è¡Œï¼‰

                if id_str:
                    print(f"id='{id_str}' ", end='')
                if class_str:
                    print(f"class='{class_str}'", end='')
                print()


è§£èª¬:

IDã€ã‚¯ãƒ©ã‚¹ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç¤º

æœ€å¾Œã®print()ã§æ”¹è¡Œ

7. ã‚¯ãƒ©ã‚¹åã®é »å‡ºãƒ©ãƒ³ã‚­ãƒ³ã‚°

        # === ã‚ˆãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹åã®ãƒ©ãƒ³ã‚­ãƒ³ã‚° ===
        print("\n9. é »å‡ºã‚¯ãƒ©ã‚¹åãƒˆãƒƒãƒ—10:")
        class_counter = {}


è§£èª¬:

ç©ºã®è¾æ›¸ã‚’ä½œæˆ

ã‚¯ãƒ©ã‚¹åã‚’ã‚­ãƒ¼ã€å‡ºç¾å›æ•°ã‚’å€¤ã¨ã—ã¦æ ¼ç´

        # å…¨ã¦ã®ã‚¿ã‚°ã‹ã‚‰ã‚¯ãƒ©ã‚¹åã‚’åé›†
        for tag in soup.find_all(class_=True):


è§£èª¬:

class_=True: ã‚¯ãƒ©ã‚¹å±æ€§ã‚’æŒã¤å…¨ã¦ã®è¦ç´ ã‚’æ¤œç´¢

classã¯Pythonã®äºˆç´„èªãªã®ã§class_ã¨ã™ã‚‹

            classes = tag.get('class', [])
            for cls in classes:
                # ã‚«ã‚¦ãƒ³ãƒˆ
                class_counter[cls] = class_counter.get(cls, 0) + 1


è§£èª¬:

1ã¤ã®è¦ç´ ãŒè¤‡æ•°ã®ã‚¯ãƒ©ã‚¹ã‚’æŒã¤å ´åˆã‚‚ã‚ã‚‹ã®ã§ãƒ«ãƒ¼ãƒ—

get(cls, 0): è¾æ›¸ã«å­˜åœ¨ã—ãªã„å ´åˆã¯0ã€å­˜åœ¨ã™ã‚‹å ´åˆã¯ç¾åœ¨ã®å€¤

+1: ã‚«ã‚¦ãƒ³ãƒˆã‚¢ãƒƒãƒ—

        # å‡ºç¾å›æ•°ã§ã‚½ãƒ¼ãƒˆ
        sorted_classes = sorted(
            class_counter.items(),
            key=lambda x: x[1],
            reverse=True
        )


è§£èª¬:

class_counter.items(): è¾æ›¸ã‚’(ã‚­ãƒ¼, å€¤)ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆã«å¤‰æ›

key=lambda x: x[1]: ã‚½ãƒ¼ãƒˆã®ã‚­ãƒ¼ï¼ˆå€¤=å‡ºç¾å›æ•°ï¼‰

reverse=True: é™é †ï¼ˆå¤šã„é †ï¼‰

        # ãƒˆãƒƒãƒ—10ã‚’è¡¨ç¤º
        for i, (cls, count) in enumerate(sorted_classes[:10], 1):
            print(f"  {i:2d}. '{cls}' - {count}å›")


è§£èª¬:

sorted_classes[:10]: æœ€åˆã®10å€‹

(cls, count): ã‚¿ãƒ—ãƒ«ã®ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ï¼ˆåˆ†è§£ï¼‰

{i:2d}: 2æ¡ã®æ•´æ•°ã¨ã—ã¦è¡¨ç¤ºï¼ˆå³å¯„ã›ï¼‰

8. HTMLã‚½ãƒ¼ã‚¹ã¨ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®ä¿å­˜

        # === HTMLã‚½ãƒ¼ã‚¹ã‚’ä¿å­˜ ===
        print("\n10. HTMLã‚½ãƒ¼ã‚¹ä¿å­˜:")
        with open('coorikuya_source.html', 'w', encoding='utf-8') as f:


è§£èª¬:

with open(): ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãï¼ˆè‡ªå‹•ã§ã‚¯ãƒ­ãƒ¼ã‚ºã•ã‚Œã‚‹ï¼‰

'w': æ›¸ãè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰

encoding='utf-8': UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰

as f: ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’fã¨ã—ã¦æ‰±ã†

            # ãã‚Œã„ã«æ•´å½¢ã—ã¦ä¿å­˜
            f.write(soup.prettify())


è§£èª¬:

soup.prettify(): HTMLã‚’ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆä»˜ãã§æ•´å½¢

f.write(): ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿

        print("  âœ“ 'coorikuya_source.html' ã«ä¿å­˜ã—ã¾ã—ãŸ")


è§£èª¬: å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã€‚

        # === ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜ ===
        driver.save_screenshot('coorikuya_screenshot.png')
        print("  âœ“ 'coorikuya_screenshot.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")


è§£èª¬:

save_screenshot(): ç¾åœ¨ã®ãƒ–ãƒ©ã‚¦ã‚¶ç”»é¢ã‚’PNGç”»åƒã¨ã—ã¦ä¿å­˜

ãƒ‡ãƒãƒƒã‚°ã‚„ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã«ä¾¿åˆ©

        print("\n" + "="*80)
        print("ã€åˆ†æå®Œäº†ï¼ã€‘")
        print("="*80)


è§£èª¬: å‡¦ç†å®Œäº†ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‚

9. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

    except Exception as e:


è§£èª¬:

tryãƒ–ãƒ­ãƒƒã‚¯å†…ã§ç™ºç”Ÿã—ãŸã‚ã‚‰ã‚†ã‚‹ä¾‹å¤–ã‚’ã‚­ãƒ£ãƒƒãƒ

as e: ä¾‹å¤–ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’eã¨ã—ã¦å–å¾—

        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


è§£èª¬: ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’å‡ºåŠ›ã€‚

        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
        import traceback
        traceback.print_exc()


è§£èª¬:

traceback: ã‚¨ãƒ©ãƒ¼ã®è©³ç´°æƒ…å ±ã‚’æ‰±ã†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

print_exc(): ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´æ‰€ã¨çµŒè·¯ï¼‰ã‚’è¡¨ç¤º

10. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†

    finally:


è§£èª¬:

tryãƒ–ãƒ­ãƒƒã‚¯ã®å¾Œã€å¿…ãšå®Ÿè¡Œã•ã‚Œã‚‹

ã‚¨ãƒ©ãƒ¼ã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšå®Ÿè¡Œã•ã‚Œã‚‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ã«æœ€é©

        # å¿…ãšãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹
        print("\n5ç§’å¾Œã«ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¾ã™...")
        time.sleep(5)


è§£èª¬: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒçµæœã‚’ç¢ºèªã§ãã‚‹ã‚ˆã†5ç§’å¾…æ©Ÿã€‚

ã€æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘æŠ€è¡“è¨˜äº‹ã€‘ç¶šã

        driver.quit()


è§£èª¬:

ãƒ–ãƒ©ã‚¦ã‚¶ã‚’å®Œå…¨ã«çµ‚äº†

close()ã¯ç¾åœ¨ã®ã‚¿ãƒ–ã®ã¿é–‰ã˜ã‚‹ãŒã€quit()ã¯å…¨ã¦ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã¨ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’çµ‚äº†

ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’é˜²ããŸã‚å¿…ãšå®Ÿè¡Œã™ã¹ã

        logger.info("ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¾ã—ãŸ")


è§£èª¬: ãƒ–ãƒ©ã‚¦ã‚¶çµ‚äº†ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã€‚

11. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†

# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã®ã¿å®Ÿè¡Œ
if __name__ == "__main__":


è§£èª¬:

__name__: ç‰¹æ®Šå¤‰æ•°ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã¯"__main__"

ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸå ´åˆã¯å®Ÿè¡Œã•ã‚Œãªã„

ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ã‚‚ä½¿ãˆã‚‹ã‚³ãƒ¼ãƒ‰ã«ãªã‚‹

    # coorikuya.comã‚’åˆ†æ
    find_specific_elements("https://www.coorikuya.com/")


è§£èª¬:

é–¢æ•°ã‚’å®Ÿè¡Œ

URLã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã™

ğŸ“Š é‡è¦ãªæ¦‚å¿µã¾ã¨ã‚

1. CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã®ç¨®é¡

ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ æ„å‘³ ä¾‹ tag ã‚¿ã‚°å div, article .class ã‚¯ãƒ©ã‚¹å .post, .item #id ID #header, #main [attr] å±æ€§ã‚’æŒã¤ [href], [datetime] [attr="value"] å±æ€§å€¤ãŒä¸€è‡´ [rel="category"] [attr*="value"] å±æ€§å€¤ã«å«ã‚€ [class*="post"] parent child å­å­« article p parent > child ç›´æ¥ã®å­ div > p

2. BeautifulSoupã®ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

# 1ã¤ã ã‘å–å¾—
element = soup.find('div')
element = soup.find('div', class_='post')
element = soup.find('div', id='main')

# è¤‡æ•°å–å¾—
elements = soup.find_all('div')
elements = soup.find_all('div', class_='post')
elements = soup.find_all('a', href=True)

# CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼ˆæ¨å¥¨ï¼‰
element = soup.select_one('.post')  # 1ã¤ã ã‘
elements = soup.select('.post')  # å…¨ã¦

# ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
text = element.text
text = element.get_text(strip=True)

# å±æ€§å–å¾—
href = element.get('href')
src = element.get('src', '')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤æŒ‡å®š


3. Seleniumã®ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

# ãƒšãƒ¼ã‚¸æ“ä½œ
driver.get(url)  # ãƒšãƒ¼ã‚¸ã‚’é–‹ã
driver.back()  # æˆ»ã‚‹
driver.forward()  # é€²ã‚€
driver.refresh()  # æ›´æ–°

# è¦ç´ æ¤œç´¢ï¼ˆ1ã¤ï¼‰
element = driver.find_element(By.ID, 'element-id')
element = driver.find_element(By.CLASS_NAME, 'class-name')
element = driver.find_element(By.CSS_SELECTOR, '.class')

# è¦ç´ æ¤œç´¢ï¼ˆè¤‡æ•°ï¼‰
elements = driver.find_elements(By.TAG_NAME, 'div')
elements = driver.find_elements(By.CSS_SELECTOR, '.post')

# è¦ç´ æ“ä½œ
element.click()  # ã‚¯ãƒªãƒƒã‚¯
element.send_keys('text')  # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›
element.clear()  # ã‚¯ãƒªã‚¢

# æƒ…å ±å–å¾—
text = element.text
value = element.get_attribute('href')


ğŸ”§ å®Ÿè·µçš„ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºä¾‹

ä¾‹1: å¾…æ©Ÿå‡¦ç†ã®æ”¹å–„

from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# æ˜ç¤ºçš„ãªå¾…æ©Ÿï¼ˆæ¨å¥¨ï¼‰
# è¦ç´ ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§æœ€å¤§10ç§’å¾…ã¤
try:
    # WebDriverWaitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆæœ€å¤§10ç§’å¾…æ©Ÿï¼‰
    wait = WebDriverWait(driver, 10)
    
    # è¦ç´ ãŒå­˜åœ¨ã™ã‚‹ã¾ã§å¾…ã¤
    element = wait.until(
        EC.presence_of_element_located((By.CLASS_NAME, 'item'))
    )
    
    # è¦ç´ ãŒã‚¯ãƒªãƒƒã‚¯å¯èƒ½ã«ãªã‚‹ã¾ã§å¾…ã¤
    element = wait.until(
        EC.element_to_be_clickable((By.ID, 'button'))
    )
    
    # è¦ç´ ãŒè¦‹ãˆã‚‹ã‚ˆã†ã«ãªã‚‹ã¾ã§å¾…ã¤
    element = wait.until(
        EC.visibility_of_element_located((By.CSS_SELECTOR, '.content'))
    )
    
except TimeoutException:
    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ï¼ˆ10ç§’çµŒã£ã¦ã‚‚è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„ï¼‰
    logger.error("è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


è§£èª¬:

time.sleep()ã¯å›ºå®šæ™‚é–“å¾…ã¤ã ã‘ã§éåŠ¹ç‡

WebDriverWaitã¯æ¡ä»¶ãŒæº€ãŸã•ã‚ŒãŸç¬é–“ã«æ¬¡ã¸é€²ã‚€ã®ã§åŠ¹ç‡çš„

ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚‚è¨­å®šã§ãã‚‹

ä¾‹2: ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜

import csv
from datetime import datetime

def save_to_csv(data_list, filename='output.csv'):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    Args:
        data_list: è¾æ›¸ã®ãƒªã‚¹ãƒˆ
        filename: ä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«å
    """
    # ç¾åœ¨æ™‚åˆ»ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã«è¿½åŠ 
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{filename.split('.')[0]}_{timestamp}.csv"
    
    # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã¯çµ‚äº†
    if not data_list:
        logger.warning("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    with open(filename, 'w', encoding='utf-8', newline='') as f:
        # æœ€åˆã®è¾æ›¸ã®ã‚­ãƒ¼ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦ä½¿ç”¨
        fieldnames = data_list[0].keys()
        
        # DictWriterã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ›¸ãè¾¼ã¿
        writer.writeheader()
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’1ã¤ãšã¤æ›¸ãè¾¼ã¿
        for data in data_list:
            writer.writerow(data)
    
    logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {filename} ({len(data_list)}ä»¶)")

# ä½¿ç”¨ä¾‹
scraped_data = [
    {'title': 'è¨˜äº‹1', 'url': 'https://example.com/1', 'date': '2025-10-04'},
    {'title': 'è¨˜äº‹2', 'url': 'https://example.com/2', 'date': '2025-10-05'},
]
save_to_csv(scraped_data, 'articles.csv')


ä¾‹3: JSONã«ä¿å­˜

import json
from datetime import datetime

def save_to_json(data, filename='output.json'):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    Args:
        data: ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸ã¾ãŸã¯ãƒªã‚¹ãƒˆï¼‰
        filename: ä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«å
    """
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{filename.split('.')[0]}_{timestamp}.json"
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(
            data,
            f,
            ensure_ascii=False,  # æ—¥æœ¬èªã‚’ãã®ã¾ã¾ä¿å­˜
            indent=2  # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆï¼ˆè¦‹ã‚„ã™ã•ï¼‰
        )
    
    logger.info(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {filename}")

# ä½¿ç”¨ä¾‹
data = {
    'url': 'https://example.com',
    'title': 'ã‚µãƒ³ãƒ—ãƒ«ãƒšãƒ¼ã‚¸',
    'items': [
        {'name': 'ã‚¢ã‚¤ãƒ†ãƒ 1', 'price': 1000},
        {'name': 'ã‚¢ã‚¤ãƒ†ãƒ 2', 'price': 2000}
    ]
}
save_to_json(data, 'scraped_data.json')


ä¾‹4: ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œ

def scrape_with_pagination(base_url, max_pages=5):
    """
    è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    Args:
        base_url: ãƒ™ãƒ¼ã‚¹URL
        max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
    """
    # ãƒ‰ãƒ©ã‚¤ãƒãƒ¼è¨­å®š
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    # å…¨çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    all_results = []
    
    try:
        # ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ«ãƒ¼ãƒ—
        for page_num in range(1, max_pages + 1):
            # ãƒšãƒ¼ã‚¸ç•ªå·ä»˜ãURLã‚’ç”Ÿæˆ
            url = f"{base_url}?page={page_num}"
            logger.info(f"ãƒšãƒ¼ã‚¸ {page_num} ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­: {url}")
            
            # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            driver.get(url)
            time.sleep(2)  # èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            
            # HTMLå–å¾—
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # è¨˜äº‹è¦ç´ ã‚’å–å¾—
            items = soup.select('.item')
            
            # è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯çµ‚äº†
            if not items:
                logger.info(f"ãƒšãƒ¼ã‚¸ {page_num} ã«è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
                break
            
            # å„è¨˜äº‹ã‚’å‡¦ç†
            for item in items:
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                title_elem = item.select_one('h2')
                title = title_elem.get_text(strip=True) if title_elem else ''
                
                # ãƒªãƒ³ã‚¯ã‚’å–å¾—
                link_elem = item.select_one('a')
                link = link_elem.get('href', '') if link_elem else ''
                
                # ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´
                result = {
                    'page': page_num,
                    'title': title,
                    'url': link
                }
                all_results.append(result)
            
            logger.info(f"ãƒšãƒ¼ã‚¸ {page_num}: {len(items)}ä»¶å–å¾—")
            
            # æ¬¡ã®ãƒšãƒ¼ã‚¸ãƒœã‚¿ãƒ³ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            next_button = soup.select_one('.pagination .next')
            if not next_button:
                logger.info("æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
                break
        
        logger.info(f"åˆè¨ˆ {len(all_results)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
        return all_results
        
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹
        driver.quit()
        logger.info("ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¾ã—ãŸ")

# ä½¿ç”¨ä¾‹
results = scrape_with_pagination("https://example.com/articles", max_pages=10)
save_to_csv(results, 'all_articles.csv')


è§£èª¬:

range(1, max_pages + 1): 1ã‹ã‚‰max_pagesã¾ã§ãƒ«ãƒ¼ãƒ—

ãƒšãƒ¼ã‚¸ã”ã¨ã«URLã‚’ç”Ÿæˆã—ã¦ã‚¢ã‚¯ã‚»ã‚¹

æ¬¡ãƒšãƒ¼ã‚¸ãƒœã‚¿ãƒ³ã®æœ‰ç„¡ã‚’ç¢ºèªã—ã¦è‡ªå‹•çµ‚äº†

ä¾‹5: ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾å¿œï¼ˆç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼‰

def scrape_infinite_scroll(url, scroll_count=5):
    """
    ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    Args:
        url: å¯¾è±¡URL
        scroll_count: ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°
    """
    # ãƒ‰ãƒ©ã‚¤ãƒãƒ¼è¨­å®š
    chrome_options = Options()
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    try:
        # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        driver.get(url)
        time.sleep(3)
        
        # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸é«˜ã•ã‚’å–å¾—
        last_height = driver.execute_script("return document.body.scrollHeight")
        
        # æŒ‡å®šå›æ•°ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
        for i in range(scroll_count):
            logger.info(f"ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« {i + 1}/{scroll_count}")
            
            # ä¸€ç•ªä¸‹ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
            time.sleep(2)
            
            # æ–°ã—ã„ãƒšãƒ¼ã‚¸é«˜ã•ã‚’å–å¾—
            new_height = driver.execute_script("return document.body.scrollHeight")
            
            # é«˜ã•ãŒå¤‰ã‚ã‚‰ãªã‘ã‚Œã°çµ‚äº†ï¼ˆã“ã‚Œä»¥ä¸Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒãªã„ï¼‰
            if new_height == last_height:
                logger.info("ã“ã‚Œä»¥ä¸Šã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§ãã¾ã›ã‚“")
                break
            
            # é«˜ã•ã‚’æ›´æ–°
            last_height = new_height
        
        # å…¨ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        items = soup.select('.item')
        
        logger.info(f"åˆè¨ˆ {len(items)}å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—ã—ã¾ã—ãŸ")
        
        # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        results = []
        for item in items:
            # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            title = item.select_one('h2')
            title_text = title.get_text(strip=True) if title else ''
            
            results.append({'title': title_text})
        
        return results
        
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        
    finally:
        driver.quit()

# ä½¿ç”¨ä¾‹
data = scrape_infinite_scroll("https://example.com", scroll_count=10)


è§£èª¬:

execute_script(): JavaScriptã‚’å®Ÿè¡Œ

scrollHeight: ãƒšãƒ¼ã‚¸å…¨ä½“ã®é«˜ã•

é«˜ã•ãŒå¤‰ã‚ã‚‰ãªããªã£ãŸã‚‰çµ‚äº†

ğŸ›¡ï¸ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

from selenium.common.exceptions import (
    TimeoutException,
    NoSuchElementException,
    ElementNotInteractableException,
    WebDriverException
)

def safe_scrape(url):
    """
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å¼·åŒ–ã—ãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    """
    driver = None
    
    try:
        # ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        driver.set_page_load_timeout(30)  # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ30ç§’
        
        # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        try:
            driver.get(url)
        except TimeoutException:
            logger.error(f"ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
            return None
        
        # è¦ç´ ã‚’å–å¾—
        try:
            wait = WebDriverWait(driver, 10)
            element = wait.until(
                EC.presence_of_element_located((By.CLASS_NAME, 'item'))
            )
        except TimeoutException:
            logger.error("è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None
        except NoSuchElementException:
            logger.error("æŒ‡å®šã•ã‚ŒãŸè¦ç´ ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return None
        
        # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        items = soup.select('.item')
        
        # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆ
        if not items:
            logger.warning("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return []
        
        # çµæœã‚’è¿”ã™
        return items
        
    except WebDriverException as e:
        # WebDriverã«é–¢ã™ã‚‹ã‚¨ãƒ©ãƒ¼
        logger.error(f"WebDriverã‚¨ãƒ©ãƒ¼: {e}")
        return None
        
    except Exception as e:
        # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
        logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None
        
    finally:
        # å¿…ãšãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’é–‰ã˜ã‚‹
        if driver:
            try:
                driver.quit()
                logger.info("ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’æ­£å¸¸ã«çµ‚äº†ã—ã¾ã—ãŸ")
            except Exception as e:
                logger.error(f"ãƒ‰ãƒ©ã‚¤ãƒãƒ¼çµ‚äº†æ™‚ã«ã‚¨ãƒ©ãƒ¼: {e}")


è§£èª¬:

å„ç¨®ã‚¨ãƒ©ãƒ¼ã‚’å€‹åˆ¥ã«ã‚­ãƒ£ãƒƒãƒ

TimeoutException: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

NoSuchElementException: è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„

WebDriverException: WebDriveré–¢é€£ã®ã‚¨ãƒ©ãƒ¼

finallyã§å¿…ãšã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

ğŸ“ ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®ä½¿ã„åˆ†ã‘

# DEBUG: è©³ç´°ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆé–‹ç™ºæ™‚ã®ã¿ï¼‰
logger.debug(f"å¤‰æ•°ã®å€¤: {variable}")

# INFO: é€šå¸¸ã®å‹•ä½œæƒ…å ±
logger.info("ãƒšãƒ¼ã‚¸ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’é–‹å§‹")

# WARNING: è­¦å‘Šï¼ˆå‡¦ç†ã¯ç¶™ç¶šï¼‰
logger.warning("è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™")

# ERROR: ã‚¨ãƒ©ãƒ¼ï¼ˆå‡¦ç†ã«å½±éŸ¿ï¼‰
logger.error("ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")

# CRITICAL: è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ åœæ­¢ãƒ¬ãƒ™ãƒ«ï¼‰
logger.critical("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ")


ğŸ¯ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1. ãƒ­ãƒœãƒƒãƒˆè¦ç´„ã‚’ç¢ºèª

import requests

def check_robots_txt(base_url):
    """
    robots.txtã‚’ç¢ºèª
    """
    robots_url = f"{base_url}/robots.txt"
    
    try:
        # robots.txtã‚’å–å¾—
        response = requests.get(robots_url, timeout=5)
        
        # æˆåŠŸã—ãŸå ´åˆ
        if response.status_code == 200:
            print(f"=== {base_url} ã® robots.txt ===")
            print(response.text)
            return response.text
        else:
            print(f"robots.txtãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}ï¼‰")
            return None
            
    except Exception as e:
        print(f"robots.txtå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# ä½¿ç”¨ä¾‹
check_robots_txt("https://example.com")


è§£èª¬:

robots.txt: ã‚µã‚¤ãƒˆé‹å–¶è€…ãŒã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã«å¯¾ã—ã¦ç¤ºã™ãƒ«ãƒ¼ãƒ«

ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‰ã«å¿…ãšç¢ºèªã™ã¹ã

2. ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’ç©ºã‘ã‚‹

import random

def polite_scraping(urls):
    """
    ç¤¼å„€æ­£ã—ã„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    """
    results = []
    
    for i, url in enumerate(urls, 1):
        logger.info(f"å‡¦ç†ä¸­ {i}/{len(urls)}: {url}")
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        data = scrape_page(url)
        results.append(data)
        
        # æœ€å¾Œã®ãƒšãƒ¼ã‚¸ä»¥å¤–ã¯å¾…æ©Ÿ
        if i < len(urls):
            # 1ï½3ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“
            wait_time = random.uniform(1.0, 3.0)
            logger.info(f"{wait_time:.1f}ç§’å¾…æ©Ÿ...")
            time.sleep(wait_time)
    
    return results


è§£èª¬:

ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’ç©ºã‘ã‚‹

ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“ã§äººé–“ã‚‰ã—ã„å‹•ä½œã‚’æ¨¡å€£

3. User-Agentã®è¨­å®š

# è‰¯ã„ä¾‹
chrome_options.add_argument(
    'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
)

# æ‚ªã„ä¾‹ï¼ˆãƒœãƒƒãƒˆã¨ã—ã¦è­˜åˆ¥ã•ã‚Œã‚‹å¯èƒ½æ€§ï¼‰
# User-Agentã‚’è¨­å®šã—ãªã„


ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

1. ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆæ’®å½±

def debug_screenshot(driver, filename_prefix='debug'):
    """
    ãƒ‡ãƒãƒƒã‚°ç”¨ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
    """
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{filename_prefix}_{timestamp}.png"
    
    # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜
    driver.save_screenshot(filename)
    logger.info(f"ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜: {filename}")
    
    return filename

# ã‚¨ãƒ©ãƒ¼æ™‚ã«ä½¿ç”¨
try:
    element = driver.find_element(By.CLASS_NAME, 'not-exist')
except NoSuchElementException:
    # ã‚¨ãƒ©ãƒ¼æ™‚ã®ç”»é¢ã‚’ä¿å­˜
    debug_screenshot(driver, 'error')
    logger.error("è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


2. HTMLæ§‹é€ ã®å‡ºåŠ›

def print_element_structure(element, indent=0):
    """
    è¦ç´ ã®æ§‹é€ ã‚’å†å¸°çš„ã«è¡¨ç¤º
    Args:
        element: BeautifulSoupè¦ç´ 
        indent: ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«
    """
    # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ
    prefix = "  " * indent
    
    # ã‚¿ã‚°åã¨ã‚¯ãƒ©ã‚¹ã‚’è¡¨ç¤º
    class_str = ' '.join(element.get('class', []))
    id_str = element.get('id', '')
    
    print(f"{prefix}<{element.name}>", end='')
    if id_str:
        print(f" id='{id_str}'", end='')
    if class_str:
        print(f" class='{class_str}'", end='')
    print()
    
    # å­è¦ç´ ã‚’å†å¸°çš„ã«è¡¨ç¤º
    for child in element.find_all(recursive=False):
        if child.name:  # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚’é™¤å¤–
            print_element_structure(child, indent + 1)

# ä½¿ç”¨ä¾‹
soup = BeautifulSoup(html, 'html.parser')
body = soup.find('body')
print_element_structure(body)


ğŸ“š ã¾ã¨ã‚

ã“ã®ã‚³ãƒ¼ãƒ‰ã§å­¦ã‚“ã ã“ã¨

ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ: å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é©åˆ‡ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

ãƒ­ã‚°è¨­å®š: ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡ŒçŠ¶æ³ã‚’è¨˜éŒ²

ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—: Chromeã‚’è‡ªå‹•æ“ä½œã™ã‚‹æº–å‚™

HTMLè§£æ: BeautifulSoupã§DOMæ§‹é€ ã‚’è§£æ

è¦ç´ æ¤œç´¢: CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚„ã‚¿ã‚°åã§è¦ç´ ã‚’æ¢ã™

ãƒ‡ãƒ¼ã‚¿æŠ½å‡º: ãƒ†ã‚­ã‚¹ãƒˆã€å±æ€§ã€ãƒªãƒ³ã‚¯ãªã©ã‚’å–å¾—

ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: try-except-finallyã§å …ç‰¢ãªã‚³ãƒ¼ãƒ‰

ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: HTMLã‚„ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

âœ… ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦å‹•ä½œã‚’ç¢ºèª

âœ… åˆ¥ã®ã‚µã‚¤ãƒˆã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è©¦ã™

âœ… CSVã‚„JSONã¸ã®ä¿å­˜æ©Ÿèƒ½ã‚’è¿½åŠ 

âœ… ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œã‚’å®Ÿè£…

âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜ã‚’å®Ÿè£…

ğŸš€ å®Ÿè·µèª²é¡Œ

åˆç´šèª²é¡Œ

# èª²é¡Œ1: h3ã‚¿ã‚°ã‚’å…¨ã¦å–å¾—ã—ã¦è¡¨ç¤ºã™ã‚‹é–¢æ•°ã‚’ä½œæˆ
# èª²é¡Œ2: ç”»åƒã®srcå±æ€§ã‚’å…¨ã¦å–å¾—ã—ã¦ãƒªã‚¹ãƒˆã«ä¿å­˜
# èª²é¡Œ3: å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜


ä¸­ç´šèª²é¡Œ

# èª²é¡Œ4: è¤‡æ•°ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
# èª²é¡Œ5: ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã«è¨˜äº‹ã‚’åˆ†é¡
# èª²é¡Œ6: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚’CSVã«ä¿å­˜


ä¸Šç´šèª²é¡Œ

# èª²é¡Œ7: ãƒ­ã‚°ã‚¤ãƒ³æ©Ÿèƒ½ã®å®Ÿè£…
# èª²é¡Œ8: å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆAjaxï¼‰ã®å–å¾—
# èª²é¡Œ9: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆSQLiteï¼‰ã¸ã®ä¿å­˜


å‚è€ƒãƒªãƒ³ã‚¯

Seleniumå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://www.selenium.dev/documentation/

BeautifulSoupå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://www.crummy.com/software/BeautifulSoup/bs4/doc/

Pythonå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://docs.python.org/ja/3/

ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ğŸ‰ è³ªå•ãŒã‚ã‚Œã°ã„ã¤ã§ã‚‚èã„ã¦ãã ã•ã„ã­ï¼

æœ€çµ‚ã€€å‹•ã‹ã›ã‚‹å…¨ä½“ã‚³ãƒ¼ãƒ‰ï¼ˆå®Œå…¨ç‰ˆï¼‰

# === å¿…è¦
# Seleniumã®åŸºæœ¬æ©Ÿèƒ½
from selenium import webdriver
# Chromeã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šç”¨
from selenium.webdriver.chrome.options import Options
# Chromeã®ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šç”¨
from selenium.webdriver.chrome.service import Service
# ChromeDriverã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ„ãƒ¼ãƒ«
from webdriver_manager.chrome import ChromeDriverManager
# HTMLè¦ç´ ã‚’æ¢ã™ãŸã‚ã®æ–¹æ³•ã‚’æŒ‡å®š
from selenium.webdriver.common.by import By
# BeautifulSoupã§HTMLè§£æ
from bs4 import BeautifulSoup
# å¾…æ©Ÿå‡¦ç†ç”¨
import time
# ãƒ­ã‚°å‡ºåŠ›ç”¨
import logging

# ãƒ­ã‚°ã®è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def find_specific_elements(url):
    """
    ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦ç´ ã‚’æ¢ç´¢ã™ã‚‹
    """
    # Chromeã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    chrome_options = Options()
    
    # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚ªãƒ•(ç”»é¢ã‚’è¡¨ç¤º)
    # chrome_options.add_argument('--headless')  # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ=ç”»é¢è¡¨ç¤º
    
    # ãã®ä»–ã®åŸºæœ¬è¨­å®š
    chrome_options.add_argument('--disable-gpu')  # GPUç„¡åŠ¹åŒ–
    chrome_options.add_argument('--no-sandbox')  # ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ç„¡åŠ¹åŒ–
    chrome_options.add_argument('--window-size=1920,1080')  # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
    
    # User-Agentã‚’è¨­å®š(ãƒœãƒƒãƒˆåˆ¤å®šå›é¿)
    chrome_options.add_argument(
        'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    )
    
    # ChromeDriverã‚’è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    service = Service(ChromeDriverManager().install())
    
    # Chromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’èµ·å‹•
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    try:
        # å¯¾è±¡URLã«ã‚¢ã‚¯ã‚»ã‚¹
        logger.info(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
        driver.get(url)
        
        # ãƒšãƒ¼ã‚¸ãŒå®Œå…¨ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§3ç§’å¾…æ©Ÿ
        time.sleep(3)
        
        # ãƒšãƒ¼ã‚¸ã®HTMLã‚½ãƒ¼ã‚¹ã‚’å–å¾—
        page_source = driver.page_source
        
        # BeautifulSoupã§è§£æ(HTMLã‚’æ‰±ã„ã‚„ã™ãã™ã‚‹)
        soup = BeautifulSoup(page_source, 'html.parser')
        
        print("\n" + "="*80)
        print("ã€coorikuya.com è©³ç´°ãªè¦ç´ æ¤œç´¢ã€‘")
        print("="*80)
        
        # === ãƒ‘ã‚¿ãƒ¼ãƒ³1: è¨˜äº‹ä¸€è¦§ã‚’æ¢ã™ ===
        print("\n1. è¨˜äº‹ä¸€è¦§å€™è£œ:")
        # ã‚ˆãã‚ã‚‹è¨˜äº‹ä¸€è¦§ã®ãƒ‘ã‚¿ãƒ¼ãƒ³(CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼)
        patterns = [
            ('article'),  # articleã‚¿ã‚°
            ('.post'),  # postã‚¯ãƒ©ã‚¹
            ('.entry'),  # entryã‚¯ãƒ©ã‚¹
            ('.blog-post'),  # blog-postã‚¯ãƒ©ã‚¹
            ('.article-item'),  # article-itemã‚¯ãƒ©ã‚¹
            ('[class*="post"]'),  # postã‚’å«ã‚€ã‚¯ãƒ©ã‚¹
            ('.card'),  # cardã‚¯ãƒ©ã‚¹
            ('.item'),  # itemã‚¯ãƒ©ã‚¹
        ]
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§è¦ç´ ã‚’æ¤œç´¢
        for pattern in patterns:
            # CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§è¦ç´ ã‚’æ¤œç´¢
            elements = soup.select(pattern)
            # è¦ç´ ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
            if elements:
                print(f"  âœ“ ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ '{pattern}': {len(elements)}å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                # æœ€åˆã®è¦ç´ ã®æ§‹é€ ã‚’è¡¨ç¤º
                if elements:
                    first = elements[0]
                    # ã‚¿ã‚°åã¨ã‚¯ãƒ©ã‚¹åã‚’å–å¾—
                    class_list = first.get('class', [])
                    print(f"    æ§‹é€ ä¾‹: <{first.name} class='{' '.join(class_list)}'>")
                    # ç›´æ¥ã®å­è¦ç´ ã‚’ç¢ºèª
                    children = first.find_all(recursive=False)
                    print(f"    ç›´æ¥ã®å­è¦ç´ : {len(children)}å€‹")
        
        # === ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™ ===
        print("\n2. ã‚¿ã‚¤ãƒˆãƒ«å€™è£œ:")
        title_patterns = [
            ('h1'),  # h1ã‚¿ã‚°
            ('h2'),  # h2ã‚¿ã‚°
            ('h2.entry-title'),  # entry-titleã‚¯ãƒ©ã‚¹ã‚’æŒã¤h2
            ('h2.post-title'),  # post-titleã‚¯ãƒ©ã‚¹ã‚’æŒã¤h2
            ('.title'),  # titleã‚¯ãƒ©ã‚¹
            ('article h2'),  # articleå†…ã®h2
            ('h2 a'),  # ãƒªãƒ³ã‚¯ã‚’å«ã‚€h2
            ('.entry-title'),  # entry-titleã‚¯ãƒ©ã‚¹
        ]
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        for pattern in title_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  âœ“ '{pattern}': {len(elements)}å€‹")
                # æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º(50æ–‡å­—ã¾ã§)
                if elements:
                    text = elements[0].get_text(strip=True)
                    print(f"    ä¾‹: {text[:50]}")
        
        # === ãƒ‘ã‚¿ãƒ¼ãƒ³3: æ—¥ä»˜ã‚’æ¢ã™ ===
        print("\n3. æ—¥ä»˜å€™è£œ:")
        date_patterns = [
            ('time'),  # timeã‚¿ã‚°
            ('.date'),  # dateã‚¯ãƒ©ã‚¹
            ('.published'),  # publishedã‚¯ãƒ©ã‚¹
            ('.entry-date'),  # entry-dateã‚¯ãƒ©ã‚¹
            ('[datetime]'),  # datetimeå±æ€§ã‚’æŒã¤è¦ç´ 
            ('.post-date'),  # post-dateã‚¯ãƒ©ã‚¹
        ]
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        for pattern in date_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  âœ“ '{pattern}': {len(elements)}å€‹")
                if elements:
                    # ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯datetimeå±æ€§ã‚’è¡¨ç¤º
                    text = elements[0].get_text(strip=True)
                    datetime_attr = elements[0].get('datetime', '')
                    print(f"    ä¾‹: {text}")
                    if datetime_attr:
                        print(f"    datetimeå±æ€§: {datetime_attr}")
        
        # === ãƒ‘ã‚¿ãƒ¼ãƒ³4: æœ¬æ–‡ãƒ»æŠœç²‹ã‚’æ¢ã™ ===
        print("\n4. æœ¬æ–‡ãƒ»æŠœç²‹å€™è£œ:")
        content_patterns = [
            ('.entry-content'),  # entry-contentã‚¯ãƒ©ã‚¹
            ('.post-content'),  # post-contentã‚¯ãƒ©ã‚¹
            ('.excerpt'),  # excerptã‚¯ãƒ©ã‚¹
            ('.summary'),  # summaryã‚¯ãƒ©ã‚¹
            ('article p'),  # articleå†…ã®p(æ®µè½)
            ('.description'),  # descriptionã‚¯ãƒ©ã‚¹
        ]
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        for pattern in content_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  âœ“ '{pattern}': {len(elements)}å€‹")
                if elements:
                    # ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—(æœ€åˆã®80æ–‡å­—)
                    text = elements[0].get_text(strip=True)
                    print(f"    ä¾‹: {text[:80]}...")
        
        # === ãƒ‘ã‚¿ãƒ¼ãƒ³5: ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ»ã‚¿ã‚°ã‚’æ¢ã™ ===
        print("\n5. ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ»ã‚¿ã‚°å€™è£œ:")
        category_patterns = [
            ('.category'),  # categoryã‚¯ãƒ©ã‚¹
            ('.tag'),  # tagã‚¯ãƒ©ã‚¹
            ('.categories'),  # categoriesã‚¯ãƒ©ã‚¹
            ('.tags'),  # tagsã‚¯ãƒ©ã‚¹
            ('a[rel="category"]'),  # relå±æ€§ãŒcategoryã®ãƒªãƒ³ã‚¯
            ('a[rel="tag"]'),  # relå±æ€§ãŒtagã®ãƒªãƒ³ã‚¯
            ('.cat-links'),  # cat-linksã‚¯ãƒ©ã‚¹
        ]
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        for pattern in category_patterns:
            elements = soup.select(pattern)
            if elements:
                print(f"  âœ“ '{pattern}': {len(elements)}å€‹")
                if elements:
                    # ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
                    print(f"    ä¾‹: {elements[0].get_text(strip=True)}")
        
        # === ãƒ‘ã‚¿ãƒ¼ãƒ³6: ç”»åƒã‚’æ¢ã™ ===
        print("\n6. ç”»åƒå€™è£œ:")
        images = soup.find_all('img')
        print(f"  ç·ç”»åƒæ•°: {len(images)}å€‹")
        
        # æœ€åˆã®3ã¤ã®ç”»åƒã‚’è¡¨ç¤º
        for i, img in enumerate(images[:3], 1):
            src = img.get('src', '')
            alt = img.get('alt', '')
            print(f"  {i}. alt='{alt[:30]}' src='{src[:60]}'")
        
        # === ãƒ‘ã‚¿ãƒ¼ãƒ³7: ãƒªãƒ³ã‚¯ã‚’æ¢ã™ ===
        print("\n7. ãƒªãƒ³ã‚¯å€™è£œ:")
        links = soup.find_all('a', href=True)
        print(f"  ç·ãƒªãƒ³ã‚¯æ•°: {len(links)}å€‹")
        
        # æœ€åˆã®5ã¤ã®ãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º
        for i, link in enumerate(links[:5], 1):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            print(f"  {i}. {text[:30]} -> {href[:50]}")
        
        # === å…¨ä½“ã®HTMLæ§‹é€ ãƒ„ãƒªãƒ¼ã‚’è¡¨ç¤º ===
        print("\n8. HTMLæ§‹é€ ãƒ„ãƒªãƒ¼(bodyç›´ä¸‹ã®è¦ç´ ):")
        body = soup.find('body')
        if body:
            # bodyã®ç›´æ¥ã®å­è¦ç´ ã‚’åˆ—æŒ™
            for i, child in enumerate(body.find_all(recursive=False), 1):
                # ã‚¯ãƒ©ã‚¹åã‚’å–å¾—
                class_str = ' '.join(child.get('class', []))
                # IDå±æ€§ã‚’å–å¾—
                id_str = child.get('id', '')
                
                # å‡ºåŠ›
                print(f"  {i}. <{child.name}> ", end='')
                if id_str:
                    print(f"id='{id_str}' ", end='')
                if class_str:
                    print(f"class='{class_str}'", end='')
                print()
        
        # === ã‚ˆãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹åã®ãƒ©ãƒ³ã‚­ãƒ³ã‚° ===
        print("\n9. é »å‡ºã‚¯ãƒ©ã‚¹åãƒˆãƒƒãƒ—10:")
        class_counter = {}
        # å…¨ã¦ã®ã‚¿ã‚°ã‹ã‚‰ã‚¯ãƒ©ã‚¹åã‚’åé›†
        for tag in soup.find_all(class_=True):
            classes = tag.get('class', [])
            for cls in classes:
                # ã‚«ã‚¦ãƒ³ãƒˆ
                class_counter[cls] = class_counter.get(cls, 0) + 1
        
        # å‡ºç¾å›æ•°ã§ã‚½ãƒ¼ãƒˆ
        sorted_classes = sorted(
            class_counter.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # ãƒˆãƒƒãƒ—10ã‚’è¡¨ç¤º
        for i, (cls, count) in enumerate(sorted_classes[:10], 1):
            print(f"  {i:2d}. '{cls}' - {count}å›")
        
        # === HTMLã‚½ãƒ¼ã‚¹ã‚’ä¿å­˜ ===
        print("\n10. HTMLã‚½ãƒ¼ã‚¹ä¿å­˜:")
        with open('coorikuya_source.html', 'w', encoding='utf-8') as f:
            # ãã‚Œã„ã«æ•´å½¢ã—ã¦ä¿å­˜
            f.write(soup.prettify())
        print("  âœ“ 'coorikuya_source.html' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # === ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜ ===
        driver.save_screenshot('coorikuya_screenshot.png')
        print("  âœ“ 'coorikuya_screenshot.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        print("\n" + "="*80)
        print("ã€åˆ†æå®Œäº†ï¼ã€‘")
        print("="*80)
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
        import traceback
        traceback.print_exc()
        
    finally:
        # å¿…ãšãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹
        print("\n5ç§’å¾Œã«ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¾ã™...")
        time.sleep(5)
        driver.quit()
        logger.info("ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¾ã—ãŸ")


# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã®ã¿å®Ÿè¡Œ
if __name__ == "__main__":
    # coorikuya.comã‚’åˆ†æ
    find_specific_elements("https://www.coorikuya.com/")


å®Ÿè¡Œæ–¹æ³•

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜(ä¾‹: analyze_coorikuya.py)
# å®Ÿè¡Œ
py analyze_coorikuya.py


ä¿®æ­£ã—ãŸãƒã‚¤ãƒ³ãƒˆ

âœ… å…¨ã¦ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã‚’è¿½åŠ 

from selenium import webdriver from selenium.webdriver.chrome.options import Options from selenium.webdriver.chrome.service import Service from webdriver_manager.chrome import ChromeDriverManager from bs4 import BeautifulSoup import time import logging

âœ… ãƒ­ã‚°è¨­å®šã‚’è¿½åŠ 

âœ… ã‚ˆã‚Šè©³ç´°ãªåˆ†æé …ç›®ã‚’è¿½åŠ 

ç”»åƒåˆ†æ

ãƒªãƒ³ã‚¯åˆ†æ

ã‚¯ãƒ©ã‚¹åãƒ©ãƒ³ã‚­ãƒ³ã‚°

âœ… çµæœã‚’ä¿å­˜

HTMLã‚½ãƒ¼ã‚¹ â†’ coorikuya_source.html

ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ â†’ coorikuya_screenshot.png

å®Ÿè¡Œã™ã‚‹ã¨å¾—ã‚‰ã‚Œã‚‹æƒ…å ±

âœ¨ ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨:

è¨˜äº‹ä¸€è¦§ã®æ§‹é€ ãŒåˆ†ã‹ã‚‹

ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—æ–¹æ³•ãŒåˆ†ã‹ã‚‹

æ—¥ä»˜ã®å–å¾—æ–¹æ³•ãŒåˆ†ã‹ã‚‹

æœ¬æ–‡ã®å–å¾—æ–¹æ³•ãŒåˆ†ã‹ã‚‹

ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å–å¾—æ–¹æ³•ãŒåˆ†ã‹ã‚‹

HTMLå…¨ä½“ãŒä¿å­˜ã•ã‚Œã‚‹

ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãŒä¿å­˜ã•ã‚Œã‚‹

ã“ã‚Œã‚’å®Ÿè¡Œã—ã¦ã¿ã¦ã€çµæœã‚’æ•™ãˆã¦ãã ã•ã„ï¼ æ¬¡ã¯å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ã‚’ä½œã‚Šã¾ã™ã‚ˆï¼ğŸâœ¨
