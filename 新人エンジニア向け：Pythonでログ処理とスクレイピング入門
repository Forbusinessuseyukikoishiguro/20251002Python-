# Pythonライブラリのインストール方法

## 1. 基本的なインストール方法

### pip を使った標準的な方法

```bash
# 基本的なインストールコマンド
pip install requests beautifulsoup4 lxml

# または1つずつ
pip install requests
pip install beautifulsoup4
pip install lxml
```

### バージョン指定インストール

```bash
# 特定バージョンを指定
pip install requests==2.31.0

# 最小バージョンを指定
pip install requests>=2.30.0

# バージョン範囲を指定
pip install requests>=2.30.0,<3.0.0
```

## 2. requirements.txt を使った一括インストール

### requirements.txt の作成

```text
# requirements.txt
requests==2.31.0
beautifulsoup4==4.12.3
lxml==5.1.0
```

### インストール実行

```bash
# requirements.txtからインストール
pip install -r requirements.txt

# アップグレードも含めて
pip install -r requirements.txt --upgrade
```

## 3. 仮想環境の作成とインストール(推奨)

### venv を使った方法

```bash
# 仮想環境を作成
python -m venv myenv

# 仮想環境を有効化
# Windows
myenv\Scripts\activate

# macOS/Linux
source myenv/bin/activate

# ライブラリをインストール
pip install requests beautifulsoup4 lxml

# 仮想環境を無効化
deactivate
```

### 仮想環境のメリット

- プロジェクトごとに独立した環境
- バージョン競合を回避
- クリーンな環境でテスト可能

## 4. インストール確認方法

```bash
# インストール済みパッケージ一覧
pip list

# 特定パッケージの情報
pip show requests

# インストール済みパッケージをrequirements.txtに出力
pip freeze > requirements.txt
```

## 5. よくあるトラブルと解決方法

### pip が古い場合

```bash
# pipをアップグレード
python -m pip install --upgrade pip

# Windows
py -m pip install --upgrade pip
```

### 権限エラーが出る場合

```bash
# ユーザー領域にインストール
pip install --user requests beautifulsoup4

# または管理者権限で実行
# Windows: 管理者としてコマンドプロンプトを実行
# macOS/Linux: sudo pip install (非推奨)
```

### パッケージが見つからない場合

```bash
# pipのキャッシュをクリア
pip cache purge

# インデックスURLを指定
pip install --index-url https://pypi.org/simple/ requests
```

## 6. Python実行環境別のインストール

### Anaconda/Conda を使っている場合

```bash
# condaでインストール
conda install requests beautifulsoup4 lxml

# conda-forgeチャンネルから
conda install -c conda-forge requests

# または混在も可能
pip install requests beautifulsoup4
```

### Pythonバージョンが複数ある場合

```bash
# Python3.10用
python3.10 -m pip install requests

# Python3.9用
python3.9 -m pip install requests

# WindowsでPython Launcher使用
py -3.10 -m pip install requests
```

## 7. 完全なセットアップ手順(新人向け)

```bash
# ステップ1: プロジェクトフォルダ作成
mkdir my_scraping_project
cd my_scraping_project

# ステップ2: 仮想環境作成
python -m venv venv

# ステップ3: 仮想環境を有効化
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate

# ステップ4: pipをアップグレード
python -m pip install --upgrade pip

# ステップ5: 必要なライブラリをインストール
pip install requests beautifulsoup4 lxml

# ステップ6: インストール確認
pip list

# ステップ7: requirements.txt作成
pip freeze > requirements.txt
```

## 8. スクレイピング用の完全なrequirements.txt

```text
# スクレイピング基本
requests==2.31.0
beautifulsoup4==4.12.3
lxml==5.1.0

# HTML/XMLパーサー(オプション)
html5lib==1.1

# ログ強化(オプション)
colorlog==6.8.2

# データ処理(必要に応じて)
pandas==2.2.0
```

## 9. インストール後の動作確認

```python
# test_install.py
try:
    import requests
    import bs4
    import lxml
    
    print("✓ requests バージョン:", requests.__version__)
    print("✓ beautifulsoup4 バージョン:", bs4.__version__)
    print("✓ lxml インストール済み")
    print("\n全てのライブラリが正常にインストールされています!")
    
except ImportError as e:
    print("✗ インストールエラー:", e)
```

実行方法:
```bash
python test_install.py
```

## 10. よくある質問

**Q: pipコマンドが見つからない**
```bash
# python -m pip を使う
python -m pip install requests

# Windowsの場合
py -m pip install requests
```

**Q: SSL証明書エラーが出る**
```bash
# 信頼されたホストとして追加
pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org requests
```

**Q: インストールが遅い**
```bash
# 国内ミラーを使用(日本)
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple requests

# またはpipの設定ファイルに追記
# ~/.pip/pip.conf (Linux/macOS) または %APPDATA%\pip\pip.ini (Windows)
[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
```

## まとめ

**最も簡単な方法:**
```bash
pip install requests beautifulsoup4 lxml
```

**推奨される方法:**
```bash
# 仮想環境を作成してから
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install requests beautifulsoup4 lxml
```

インストールでつまずいている部分があれば、具体的なエラーメッセージを教えてください!


# 新人エンジニア向け：Pythonでログ処理とスクレイピング入門

Seleniumドライバーを使わない軽量なスクレイピング方法を、実践的なコード例とともに解説します。

## 1. 基本的なスクレイピング(requests + BeautifulSoup)

最も一般的で軽量な方法です。

```python
import requests
from bs4 import BeautifulSoup
import logging

# ログ設定
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraping.log'),
        logging.StreamHandler()
    ]
)

def scrape_website(url):
    try:
        logging.info(f"スクレイピング開始: {url}")
        
        # リクエスト送信
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # HTML解析
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # DOM操作の例
        # タイトル取得
        title = soup.find('title')
        if title:
            logging.info(f"タイトル: {title.text}")
        
        # 特定のクラスを持つ要素を取得
        articles = soup.find_all('article', class_='post')
        logging.info(f"記事数: {len(articles)}")
        
        # リンク抽出
        links = soup.find_all('a', href=True)
        for link in links[:5]:  # 最初の5件
            logging.info(f"リンク: {link['href']}")
        
        return soup
        
    except requests.exceptions.RequestException as e:
        logging.error(f"リクエストエラー: {e}")
        return None
    except Exception as e:
        logging.error(f"予期しないエラー: {e}")
        return None
```

## 2. DOM処理の実践例

```python
from bs4 import BeautifulSoup

def parse_html_structure(html_content):
    """HTML構造を解析してデータ抽出"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # CSS Selectorを使用
    # クラス指定
    items = soup.select('.item-list .item')
    
    # ID指定
    main_content = soup.select_one('#main-content')
    
    # 属性で検索
    data_items = soup.find_all(attrs={'data-type': 'product'})
    
    # 階層構造でのナビゲーション
    for item in items:
        # 親要素
        parent = item.parent
        
        # 子要素
        children = item.find_all('div', recursive=False)  # 直接の子のみ
        
        # 兄弟要素
        next_sibling = item.find_next_sibling()
        
        # テキスト抽出(タグ除去)
        text = item.get_text(strip=True)
        
        # 属性取得
        item_id = item.get('id', 'no-id')
        
        logging.info(f"アイテム: {text[:50]}")
    
    return items
```

## 3. 構造化されたログ処理

```python
import json
from datetime import datetime

class ScrapingLogger:
    """スクレイピング専用ロガー"""
    
    def __init__(self, log_file='scraping_data.jsonl'):
        self.log_file = log_file
        self.logger = logging.getLogger(__name__)
    
    def log_scraped_data(self, url, data, status='success'):
        """構造化データをJSON Lines形式で記録"""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'url': url,
            'status': status,
            'data': data,
            'items_count': len(data) if isinstance(data, list) else 1
        }
        
        # JSON Lines形式で追記
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        
        self.logger.info(f"データ記録完了: {url} ({log_entry['items_count']}件)")
    
    def read_logs(self):
        """ログを読み込んで分析"""
        logs = []
        try:
            with open(self.log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    logs.append(json.loads(line))
        except FileNotFoundError:
            self.logger.warning("ログファイルが見つかりません")
        
        return logs

# 使用例
logger = ScrapingLogger()

# スクレイピングとログ記録
url = "https://example.com"
scraped_data = [
    {'title': '商品A', 'price': '1000円'},
    {'title': '商品B', 'price': '2000円'}
]
logger.log_scraped_data(url, scraped_data)
```

## 4. 実践的な完全例

```python
import requests
from bs4 import BeautifulSoup
import logging
import time
from typing import List, Dict

class SimpleScraper:
    """Seleniumを使わないシンプルなスクレイパー"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # ログ設定
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def fetch_page(self, url: str) -> BeautifulSoup:
        """ページを取得してBeautifulSoupオブジェクトを返す"""
        try:
            self.logger.info(f"ページ取得: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            return BeautifulSoup(response.text, 'html.parser')
        
        except Exception as e:
            self.logger.error(f"ページ取得失敗: {url} - {e}")
            return None
    
    def extract_product_data(self, soup: BeautifulSoup) -> List[Dict]:
        """商品データを抽出(例)"""
        products = []
        
        # DOM要素を検索
        product_elements = soup.select('.product-item')
        
        for element in product_elements:
            try:
                product = {
                    'name': element.select_one('.product-name').get_text(strip=True),
                    'price': element.select_one('.price').get_text(strip=True),
                    'link': element.select_one('a')['href'] if element.select_one('a') else None
                }
                products.append(product)
                self.logger.debug(f"商品抽出: {product['name']}")
                
            except AttributeError as e:
                self.logger.warning(f"要素が見つかりません: {e}")
                continue
        
        self.logger.info(f"抽出完了: {len(products)}件")
        return products
    
    def scrape_with_pagination(self, base_url: str, max_pages: int = 5):
        """ページネーション対応スクレイピング"""
        all_products = []
        
        for page in range(1, max_pages + 1):
            url = f"{base_url}?page={page}"
            soup = self.fetch_page(url)
            
            if not soup:
                break
            
            products = self.extract_product_data(soup)
            all_products.extend(products)
            
            # 次のページが存在するか確認
            next_button = soup.select_one('.pagination .next')
            if not next_button:
                self.logger.info("最終ページに到達")
                break
            
            time.sleep(1)  # 礼儀正しくリクエスト間隔を空ける
        
        return all_products

# 使用例
if __name__ == "__main__":
    scraper = SimpleScraper()
    
    # 単一ページのスクレイピング
    soup = scraper.fetch_page("https://example.com")
    if soup:
        products = scraper.extract_product_data(soup)
        print(f"取得した商品数: {len(products)}")
```

## 5. よく使うDOM操作パターン

```python
# テーブルデータの抽出
def extract_table_data(soup):
    table = soup.find('table', {'id': 'data-table'})
    rows = []
    
    for tr in table.find_all('tr')[1:]:  # ヘッダー行をスキップ
        cells = tr.find_all('td')
        row_data = [cell.get_text(strip=True) for cell in cells]
        rows.append(row_data)
    
    return rows

# JSONデータを含むscriptタグから抽出
def extract_json_from_script(soup):
    import re
    
    script_tag = soup.find('script', text=re.compile('var data ='))
    if script_tag:
        json_text = re.search(r'var data = ({.*?});', script_tag.string, re.DOTALL)
        if json_text:
            data = json.loads(json_text.group(1))
            return data
    return None
```

## まとめ

**Seleniumが不要なケース:**
- 静的HTMLサイト
- APIが利用可能
- JavaScriptが不要

**必要なライブラリ:**
- `requests`: HTTP通信
- `beautifulsoup4`: HTML解析
- `lxml`: 高速パーサー(オプション)

**ログのベストプラクティス:**
- 構造化ログ(JSON)で記録
- エラーハンドリングを徹底
- スクレイピング結果を追跡可能に

何か特定のサイトやユースケースについて、より詳しく知りたいことはありますか?
