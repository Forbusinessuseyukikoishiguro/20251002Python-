# 【Python先生の解説】素晴らしい！分析成功です！🎉

完璧に動作しましたね！分析結果から、coorikuya.comの構造が分かりました。
次は**実際のスクレイピングコード**を作りましょう！

## 分析結果のまとめ

```
✅ 記事一覧: .item クラス (10個)
✅ タイトル: h1, h2 タグ
✅ 本文: article p 
✅ 画像: 14個
✅ リンク: 64個
```

## coorikuya.com 完全スクレイピングコード

```python
# === 必要なライブラリをインポート ===
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import logging
import json
import csv
from datetime import datetime

# ログ設定
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraping.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class CoorikuyaScraper:
    """coorikuya.com専用スクレイパークラス"""
    
    def __init__(self, headless=False):
        """
        初期化
        Args:
            headless: ヘッドレスモードで実行するか(True=画面非表示)
        """
        # Chromeオプションを設定
        self.chrome_options = Options()
        
        # ヘッドレスモード設定
        if headless:
            self.chrome_options.add_argument('--headless')
        
        # 基本設定
        self.chrome_options.add_argument('--disable-gpu')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--window-size=1920,1080')
        
        # User-Agent設定
        self.chrome_options.add_argument(
            'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        
        # ドライバーを初期化
        self.driver = None
        
        # スクレイピング結果を保存するリスト
        self.results = []
    
    def setup_driver(self):
        """ドライバーをセットアップ"""
        # ChromeDriverを自動インストール
        service = Service(ChromeDriverManager().install())
        
        # ドライバーを起動
        self.driver = webdriver.Chrome(
            service=service,
            options=self.chrome_options
        )
        
        # 暗黙的な待機時間を設定
        self.driver.implicitly_wait(10)
        
        logger.info("ドライバーのセットアップ完了")
    
    def scrape_top_page(self, url="https://www.coorikuya.com/"):
        """
        トップページをスクレイピング
        Args:
            url: スクレイピングするURL
        """
        try:
            # ドライバーをセットアップ
            self.setup_driver()
            
            # ページにアクセス
            logger.info(f"アクセス中: {url}")
            self.driver.get(url)
            
            # ページ読み込み待機
            time.sleep(3)
            
            # ページタイトルを取得
            page_title = self.driver.title
            logger.info(f"ページタイトル: {page_title}")
            
            # HTMLソースを取得
            page_source = self.driver.page_source
            
            # BeautifulSoupで解析
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # === 基本情報を取得 ===
            data = {
                'url': url,
                'title': page_title,
                'scraped_at': datetime.now().isoformat(),
            }
            
            # h1タグを取得
            h1 = soup.find('h1')
            if h1:
                data['h1'] = h1.get_text(strip=True)
                logger.info(f"H1: {data['h1']}")
            
            # h2タグを取得(全て)
            h2_tags = soup.find_all('h2')
            data['h2_list'] = [h2.get_text(strip=True) for h2 in h2_tags]
            logger.info(f"H2タグ数: {len(data['h2_list'])}")
            
            # 本文を取得
            article_p = soup.select('article p')
            if article_p:
                data['description'] = article_p[0].get_text(strip=True)
                logger.info(f"説明文: {data['description'][:50]}...")
            
            # === 地域別リンクを取得 ===
            items = soup.select('.item')
            logger.info(f"地域アイテム数: {len(items)}")
            
            # 地域リンクを格納するリスト
            area_links = []
            
            # 各アイテムからリンク情報を抽出
            for i, item in enumerate(items, 1):
                # リンク要素を探す
                link = item.find('a', class_='main-link')
                
                if link:
                    # href属性を取得
                    href = link.get('href', '')
                    
                    # リンクテキストを取得
                    link_text = link.get_text(strip=True)
                    
                    # 画像を探す
                    img = item.find('img')
                    img_src = img.get('src', '') if img else ''
                    img_alt = img.get('alt', '') if img else ''
                    
                    # データを格納
                    area_data = {
                        'index': i,
                        'text': link_text,
                        'url': href,
                        'full_url': f"https://www.coorikuya.com{href}" if href.startswith('/') else href,
                        'image_src': img_src,
                        'image_alt': img_alt
                    }
                    
                    area_links.append(area_data)
                    logger.info(f"{i}. {link_text} -> {href}")
            
            # データに追加
            data['area_links'] = area_links
            data['area_count'] = len(area_links)
            
            # === 全リンクを取得 ===
            all_links = soup.find_all('a', href=True)
            data['total_links'] = len(all_links)
            
            # ナビゲーションリンクを取得
            nav_links = []
            nav = soup.find('nav')
            if nav:
                nav_anchors = nav.find_all('a', href=True)
                for a in nav_anchors:
                    nav_links.append({
                        'text': a.get_text(strip=True),
                        'url': a.get('href')
                    })
            
            data['nav_links'] = nav_links
            
            # === 画像を取得 ===
            images = soup.find_all('img')
            data['total_images'] = len(images)
            
            # 結果をリストに追加
            self.results.append(data)
            
            logger.info("スクレイピング完了")
            
            return data
            
        except Exception as e:
            logger.error(f"エラーが発生しました: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        finally:
            # ドライバーを閉じる
            if self.driver:
                self.driver.quit()
                logger.info("ブラウザを閉じました")
    
    def save_to_json(self, filename='coorikuya_data.json'):
        """
        結果をJSONファイルに保存
        Args:
            filename: 保存するファイル名
        """
        try:
            # JSONファイルに保存(日本語を読める形式で)
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(
                    self.results,
                    f,
                    ensure_ascii=False,
                    indent=2
                )
            logger.info(f"JSONファイルに保存: {filename}")
            
        except Exception as e:
            logger.error(f"JSON保存エラー: {e}")
    
    def save_to_csv(self, filename='coorikuya_areas.csv'):
        """
        地域リンク情報をCSVファイルに保存
        Args:
            filename: 保存するファイル名
        """
        try:
            # 最新の結果から地域リンクを取得
            if not self.results:
                logger.warning("保存するデータがありません")
                return
            
            area_links = self.results[-1].get('area_links', [])
            
            if not area_links:
                logger.warning("地域リンクがありません")
                return
            
            # CSVファイルに書き込み
            with open(filename, 'w', encoding='utf-8', newline='') as f:
                # ヘッダーを定義
                fieldnames = ['index', 'text', 'url', 'full_url', 'image_src', 'image_alt']
                
                # CSVライターを作成
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                # ヘッダー行を書き込み
                writer.writeheader()
                
                # データ行を書き込み
                for area in area_links:
                    writer.writerow(area)
            
            logger.info(f"CSVファイルに保存: {filename} ({len(area_links)}件)")
            
        except Exception as e:
            logger.error(f"CSV保存エラー: {e}")
    
    def print_summary(self):
        """スクレイピング結果のサマリーを表示"""
        if not self.results:
            print("スクレイピング結果がありません")
            return
        
        data = self.results[-1]
        
        print("\n" + "="*80)
        print("【スクレイピング結果サマリー】")
        print("="*80)
        
        print(f"\nURL: {data.get('url')}")
        print(f"タイトル: {data.get('title')}")
        print(f"H1: {data.get('h1')}")
        print(f"説明文: {data.get('description', '')[:100]}...")
        
        print(f"\n地域数: {data.get('area_count', 0)}個")
        print(f"総リンク数: {data.get('total_links', 0)}個")
        print(f"総画像数: {data.get('total_images', 0)}個")
        
        print("\n【地域リスト】")
        for area in data.get('area_links', [])[:10]:
            print(f"{area['index']:2d}. {area['text']}")
        
        if data.get('area_count', 0) > 10:
            print(f"... 他 {data.get('area_count') - 10}個")
        
        print("\n" + "="*80)


def main():
    """メイン実行関数"""
    # スクレイパーを作成(headless=Falseで画面表示)
    scraper = CoorikuyaScraper(headless=False)
    
    # トップページをスクレイピング
    result = scraper.scrape_top_page()
    
    if result:
        # サマリーを表示
        scraper.print_summary()
        
        # JSONファイルに保存
        scraper.save_to_json('coorikuya_data.json')
        
        # CSVファイルに保存
        scraper.save_to_csv('coorikuya_areas.csv')
        
        print("\n✓ スクレイピング完了！")
        print("✓ coorikuya_data.json - 全データ")
        print("✓ coorikuya_areas.csv - 地域リンク一覧")
        print("✓ scraping.log - ログファイル")


# 実行
if __name__ == "__main__":
    main()
```

## 実行方法

```bash
# ファイル名を scrape_coorikuya.py として保存
py scrape_coorikuya.py
```

## 出力されるファイル

1. **coorikuya_data.json** - 全データ（JSON形式）
2. **coorikuya_areas.csv** - 地域リンク一覧（CSV形式）
3. **scraping.log** - ログファイル

## このコードでできること

✅ トップページの基本情報を取得
✅ H1, H2タグを取得
✅ 説明文を取得
✅ 地域別リンク（10個）を全て取得
✅ リンクURL、画像情報を取得
✅ JSON形式で保存
✅ CSV形式で保存（Excelで開ける）
✅ ログファイルに記録

## 次のステップ

このコードを実行して、CSVファイルをExcelで開いてみてください！
地域別の猫カフェリンクが一覧で見られますよ！🐱✨

何か追加したい機能や、他のページもスクレイピングしたい場合は教えてくださいね！
